\section{Introduction}
Modern AI agents are increasingly reliant on external tools to extend their capabilities beyond their parametric knowledge. The Model Context Protocol (MCP) \cite{liu2024promptinjectionattackllmintegrated}, as an emerging standard, allows servers to dynamically register tools with agents, significantly enhancing their adaptability and functionality in complex environments. In this interaction paradigm, the core basis for an agent to select and invoke a tool is solely the semantic information, such as the tool's name, description, and expected return content. A critical and widespread assumption underpinning this architecture is that the MCP server and the tool metadata it provides are trustworthy. However, this assumption is tenuous in practical, open deployment environments where services are often provided by third parties, an attacker can easily deploy a malicious MCP server, creating a significant yet overlooked attack surface. Although the security of AI agents has garnered widespread attention, existing research predominantly focuses on traditional attack surfaces like prompt injection \cite{yu2024assessingpromptinjectionrisks} and privilege escalation \cite{Yan_2025}. These studies largely overlook a systematic analysis of the emerging attack surface at the tool semantic layer (i.e., names, descriptions, and return content). The design of the MCP protocol itself, while innovative in facilitating tool integration, also does not adequately consider the risk of malicious manipulation of tool metadata, lacking effective mechanisms for authenticity verification \cite{croce2025trivialtrojansminimalmcp}.

This research gap is particularly concerning given the demonstrated vulnerability of LLMs to semantic manipulation in other contexts. Prior work has shown that LLMs can be misled through carefully crafted instructions \cite{zhang2024instructionbackdoorattackscustomized} and that their tool-selection mechanisms are sensitive to description wording \cite{blankenstein2025biasbustersuncoveringmitigatingtool}. Building on these observations, we posit the following research hypotheses: \textbf{First (H1),} an attacker need not modify the tool implementation; by manipulating the tool metadata (specifically \texttt{name}, \texttt{desc}, and \texttt{out}), they can induce the agent to invoke a malicious tool and realize their adversarial objectives. \textbf{Second (H2),} leveraging a Large Language Model (LLM)-driven automated genetic algorithm \cite{zhang2025systematicsurveylargelanguage}, guided by semantic embeddings \cite{yan2025embedx}, can efficiently generate highly successful and stealthy attack payloads that evade simple detection heuristics.

To validate these hypotheses, this paper designs a comprehensive experimental evaluation framework. We construct a controlled test environment where an attacker has full control over MCP tool metadata and formally define four categories of attack scenarios with quantifiable metrics. Furthermore, we propose the Semantic MCP Tool Hijacking (SMTH) framework to automatically generate and optimize adversarial tool metadata. This study represents the first systematic investigation of security risks at the tool semantic layer of the MCP protocol, serving as a critical wake-up call for building more robust tool invocation mechanisms for agents in untrusted environments.

The main contributions of this paper are summarized as follows:
\begin{itemize}
    \item We conduct the first systematic analysis of security risks at the semantic layer of the Model Context Protocol (MCP) tools, revealing a new attack surface that emerges with the integration of third-party MCP tools in open agent ecosystems.
    \item We propose the Semantic MCP Tool Hijacking (SMTH) framework, which automatically generates and optimizes adversarial MCP tool metadata (\texttt{name}, \texttt{desc}, and \texttt{out}) to systematically study how semantic-layer attacks can be constructed and evaluated.
    \item We establish a unified evaluation framework covering four adversarial objectives: resource exhaustion, task failure, backdoor injection, and information leakage, with clearly defined and quantifiable metrics.
    \item We perform comprehensive experiments on the LiveMCPBench benchmark to validate the feasibility and real-world impact of semantic-layer attacks on MCP-powered agents.
\end{itemize}
