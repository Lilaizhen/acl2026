% \section{Introduction}

% Large language models are evolving from isolated dialogue systems into autonomous agents that perceive environments and execute complex tasks~\cite{yao2023react,schick2024toolformer}. This paradigm shift is driven by the advancement of tool use capabilities~\cite{qin2024toolllm}. The Model Context Protocol (MCP) establishes a standardized framework to mitigate the fragmentation of tool ecosystems~\cite{mcp2024anthropic}. Often described as a universal interface for artificial intelligence, this protocol allows agents to access data sources and third-party services through a unified connection layer. These advancements broaden the operational scope of agents but introduce significant security risks through complex third-party dependencies~\cite{hou2025model}.

% Existing security research primarily targets user-to-agent attacks where an adversarial user employs jailbreaking or direct prompt injection~\cite{zou2023universal,chao2023jailbreaking}. The Model Context Protocol ecosystem introduces a distinct scenario involving benign users and agents that utilize tools from unverified third-party registries~\cite{hou2025model}. We identify a critical vulnerability in the architecture as agents rely on semantic matching for tool selection. Adversaries can manipulate tool metadata such as names and descriptions to deceive the agent into selecting a malicious component~\cite{shi2025toolhijacker,sneh2025tooltweak,mo2025attractive}. The attacker then exploits the output payload of the tool to hijack subsequent agent behavior. This mechanism constitutes a novel supply chain attack vector distinct from passive indirect injection~\cite{greshake2023more,li2025dissonances}.

% This attack vector diverges from traditional software vulnerabilities by operating as a semantic injection. Success requires balancing two interdependent objectives. The first is attraction, where tool metadata must employ persuasive strategies to ensure selection among numerous candidates~\cite{mo2025attractive}. The second is manipulation, where the output payload must bypass safety guardrails and alter the reasoning trajectory. We frame this challenge as an adversarial optimization problem targeting the planning capabilities of the agent.

% We propose \methodname{} to systematically evaluate this risk. This black-box optimization framework utilizes execution traces to generate adversarial tools. We model the attack as a two-phase process. The attraction phase optimizes tool metadata to maximize the probability of selection. The manipulation phase subsequently employs an Analyzer-Optimizer architecture. This component utilizes historical execution traces as feedback to iteratively refine the attack payload. Our approach advances beyond traditional genetic algorithms by incorporating a diagnostic Analyzer. This module distinguishes between failure modes such as ambiguity and safety refusals to enable targeted mutations.

% Our contributions are summarized as follows: \begin{itemize} \item We formalize the semantic supply chain attack surface within the Model Context Protocol, defining a two-phase Attraction-Manipulation attack lifecycle. We categorize four distinct threat scenarios: Cognitive Denial of Service (C-DoS), Information Exfiltration (IE), Environment Integrity Compromise (EIC), and Reasoning Derailment (RD). \item We introduce \methodname{}, a black-box framework utilizing an Analyzer-Optimizer architecture. By leveraging execution traces, it systematically optimizes tool metadata to attract agents and crafts payloads to hijack their reasoning. \item We provide extensive empirical evidence on LiveMCPBench across five frontier models. Evaluations show \methodname{} inflates token costs by $32.4\times$ and transfers effectively to unseen models. We further demonstrate that standard defenses like perplexity heuristics fail to detect these exploits. \end{itemize}

\section{Introduction}

Large language models (LLMs) are evolving from standalone conversational systems into autonomous agents capable of perceiving environments and executing complex tasks through external tools~\cite{yao2023react,schick2024toolformer}. The Model Context Protocol (MCP) provides a unified interface that integrates heterogeneous data sources and third-party services, substantially expanding the functional scope of agentic systems~\cite{mcp2024anthropic}. However, this heavy reliance on third-party tools also introduces new security risks, particularly when tool sources lack systematic vetting and auditing~\cite{hou2025model}.

Existing security research on LLMs has predominantly focused on \emph{user-to-agent} attacks, such as jailbreaking and direct prompt injection, where a malicious user explicitly manipulates model behavior~\cite{zou2023universal,chao2023jailbreaking}. In contrast, the MCP setting gives rise to a fundamentally different threat model: both the user and the agent may behave benignly, while the agent autonomously selects and invokes tools from third-party registries. In this process, tool selection relies heavily on semantic metadata such as tool names and descriptions, which can be exploited by adversaries to craft deceptively attractive tools that are preferentially selected by the agent~\cite{shi2025toolhijacker,sneh2025tooltweak,mo2025attractive}.

More critically, the attack does not end at tool selection. Once a malicious tool is invoked, its return content is treated by the agent as \emph{trusted environment feedback} rather than untrusted textual input. By embedding control information disguised as system states or execution results into tool responses, an attacker can hijack the agent's subsequent reasoning, tool usage, and actions. Such attacks constitute a new class of \emph{semantic supply-chain attacks} that operate through trusted tool interfaces, and are fundamentally distinct from indirect prompt injection attacks~\cite{greshake2023more,li2025dissonances}.

Successfully mounting a semantic tool-based attack requires satisfying two objectives simultaneously. First, the malicious tool must be sufficiently \emph{attractive} to ensure it is selected among competing alternatives. Second, after invocation, its return payload must be capable of \emph{manipulating} the agent's reasoning trajectory. These objectives are inherently decoupled in practice: a tool that is effective at manipulation is useless if it is never selected, while a frequently selected tool may still fail to influence the agent after invocation. We therefore model semantic tool attacks as a two-stage adversarial optimization problem targeting the planning and execution mechanisms of agentic systems.

Based on this formulation, we propose \textbf{A2M (Attraction-to-Manipulation)}, a black-box attack framework for generating malicious MCP tools. A2M explicitly decomposes the attack process into attraction and manipulation stages, and introduces an \emph{Analyzer--Optimizer} architecture that leverages execution traces to diagnose attack failures (e.g., semantic ambiguity or safety refusal), enabling targeted iterative refinement rather than random search.

We evaluate A2M on LiveMCPBench across five frontier models. Experimental results show that A2M can reliably hijack agent behavior under multiple threat scenarios, amplify internal token consumption by up to $32.4\times$, and generalize effectively to previously unseen models. Moreover, we find that existing perplexity-based detection methods and lightweight auditing mechanisms fail to reliably defend against these attacks.

Our main contributions are summarized as follows:
\begin{itemize}
    \item We identify and formalize a previously underexplored \emph{semantic supply-chain attack surface} in MCP-based agent systems, and propose a two-stage Attraction--Manipulation attack model with systematic security analysis.
    \item We propose the \textbf{A2M} framework, which jointly optimizes malicious tool metadata and return payloads via an Analyzer--Optimizer architecture, enabling stable post-selection reasoning hijacking.
    \item Extensive experiments on LiveMCPBench demonstrate the severity, cross-model transferability, and poor detectability of semantic tool-based attacks under existing defenses.
\end{itemize}
