\section{Introduction}
Modern AI agents are increasingly reliant on external tools to extend their capabilities beyond their parametric knowledge. The Model Context Protocol (MCP) \cite{liu2024promptinjectionattackllmintegrated}, as an emerging standard, allows servers to dynamically register tools with agents, significantly enhancing their adaptability and functionality in complex environments. In this interaction paradigm, the core basis for an agent to select and invoke a tool is solely the semantic information—such as the tool's name, description, and expected return content. \textbf{A critical and widespread assumption underpinning this architecture is that the MCP server and the tool metadata it provides are trustworthy.} However, this assumption is tenuous in practical, open deployment environments where services are often provided by third parties—an attacker can easily deploy a malicious MCP server, creating a significant yet overlooked attack surface. Although the security of AI agents has garnered widespread attention, existing research predominantly focuses on traditional attack surfaces like prompt injection \cite{yu2024assessingpromptinjectionrisks} and privilege escalation \cite{Yan_2025}. These studies \textbf{largely overlook a systematic analysis of the emerging attack surface at the tool semantic layer (i.e., names, descriptions, and return content).} The design of the MCP protocol itself, while innovative in facilitating tool integration, also does not adequately consider the risk of malicious manipulation of tool metadata, lacking effective mechanisms for authenticity verification \cite{croce2025trivialtrojansminimalmcp}.

This research gap is particularly concerning given the demonstrated vulnerability of LLMs to semantic manipulation in other contexts. Prior work has shown that LLMs can be misled through carefully crafted instructions \cite{zhang2024instructionbackdoorattackscustomized} and that their tool-selection mechanisms are sensitive to description wording \cite{blankenstein2025biasbustersuncoveringmitigatingtool}. Building on these observations, we posit the following research hypotheses: \textbf{First (H1),} an attacker can successfully induce an agent to invoke a malicious tool by crafting its name and description, without modifying the underlying tool code, thereby achieving attack goals such as resource waste, information leakage, backdoor implantation, or task failure induction. \textbf{Second (H2),} leveraging a Large Language Model (LLM)-driven automated genetic algorithm \cite{zhang2025systematicsurveylargelanguage}, guided by semantic embeddings \cite{yan2025embedx}, can efficiently generate highly successful and stealthy attack payloads that evade simple detection heuristics.

To validate these hypotheses, this paper designs a comprehensive experimental evaluation framework. We construct a controlled test environment where an attacker has full control over MCP tool metadata and formally define four categories of attack scenarios with quantifiable metrics. Furthermore, we propose an innovative Semantic MCP Tool Hijacking (\methodacronym) framework for automatically generating and optimizing attack payloads. This study represents the first systematic investigation of security risks at the tool semantic layer of the MCP protocol, serving as a critical wake-up call for building more robust tool invocation mechanisms for agents in untrusted environments.

