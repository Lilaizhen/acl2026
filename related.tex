\section{Related work}

\subsection{AI Agents and External Tool Interaction Mechanisms}

The paradigm of enabling large language models to use external tools has emerged as a key path to enhancing their capabilities and grounding their responses in real-world data. Early works like \citet{schick2023toolformerlanguagemodelsteach} and \citet{qin2023tool} pioneered this direction, enabling LLMs to understand and call APIs through fine-tuning or instruction tuning. Subsequently, reasoning and acting frameworks such as ReAct \cite{yao2023reactsynergizingreasoningacting} and comprehensive platforms like AutoGPT \cite{yang2023autogptonlinedecisionmaking} and LangChain \cite{langchain2023} further advanced agent architectures by supporting the dynamic selection and execution of appropriate tools from a toolkit based on user queries. A \textbf{common cornerstone} of these methods is their heavy reliance on the natural language description (metadata) of tools for semantic matching to decide which tool to invoke. However, these studies generally \textbf{implicitly assume a crucial premise: that tool descriptions are accurate and trustworthy}, focusing instead on improving selection accuracy or expanding tool repertoire. Our work directly challenges this premise by thoroughly investigating the security threats that arise when tool metadata itself becomes an attack vector in these established interaction paradigms.

\subsection{Attacks Targeting AI Agents}

The expanding capabilities of AI agents have correspondingly broadened their attack surface, spawning substantial security research. A prominent category comprises \textbf{prompt injection} attacks  \cite{yu2024assessingpromptinjectionrisks}, where adversaries use carefully crafted user inputs to hijack the model's instruction-following mechanism, leading it to deviate from its intended behavior. Another significant line of research focuses on \textbf{privilege misuse} \cite{kim2025promptflowintegrityprevent}, where an agent, though properly authorized, is induced through social engineering or logical flaws to perform dangerous operations it should not execute (e.g., deleting files, sending emails). More sophisticated threats include model jailbreaking \cite{zhang2024instructionbackdoorattackscustomized} and data extraction attacks \cite{shi2025promptinjectionattacktool}. A \textbf{fundamental limitation} of these valuable studies is that they primarily situate the attack surface on the \textbf{user input} or the \textbf{tool implementation layer}, while assuming the "instruction manual" (i.e., the metadata) provided by the tool is genuine and reliable. Our research reveals a neglected blind spot in this threat landscape: even if a tool's implementation code is entirely harmless, its \textbf{maliciously altered name and description} are sufficient to pose a serious and direct threat by subverting the agent's decision-making process at the most fundamental level.

\subsection{Automated Attack Generation Techniques}
The automation of adversarial example generation has been extensively explored in both computer vision and NLP domains. Traditional methods often rely on gradient-based approaches \cite{alzantot-etal-2018-generating} or genetic algorithms \cite{alzantot2019genattackpracticalblackboxattacks}. More recently, LLMs have been harnessed for this purpose, demonstrating remarkable capability in generating semantically meaningful adversarial examples \cite{paulus2025advprompterfastadaptiveadversarial}. Our work draws inspiration from these advances but applies them to the novel domain of tool metadata manipulation. Unlike prior work that focused on perturbing input data, we leverage a hybrid approach combining LLMs' creative generation capabilities with the structured search of genetic algorithms, further refined using semantic embeddings to ensure the generated attacks remain semantically coherent and stealthy while achieving their malicious objectives.

\subsection{Our Contributions and Differentiation}
Compared to existing research, this paper makes unique contributions across several key dimensions. In terms of the attack surface, while existing literature primarily focuses on traditional vectors such as user input, tool implementation code, or execution privileges, our work pioneers a systematic investigation of the metadata layer—comprising tool names, descriptions, and return content—establishing it for the first time as a primary attack vector against AI agents.

At the level of attack generation techniques, existing methods largely rely on manually crafted templates or gradient-based perturbations, lacking adaptability and intelligence. We innovatively propose the Semantic MCP Tool Hijacking (\methodacronym) framework based on large language models. This framework, guided by semantic embeddings for crossover and mutation operations, can efficiently evolve attack payloads that simultaneously achieve high success rates and strong stealth.

Methodologically, existing studies are often confined to qualitative analyses or simple case studies in specific scenarios, lacking a systematic evaluation framework. We construct a formal threat model and four comprehensive attack scenarios, establishing a quantifiable evaluation index system that provides an important benchmark for future research in this emerging field.

In summary, by introducing an innovative automated attack generation framework and conducting systematic empirical evaluation, this paper not only fills a critical gap in AI agent security research concerning attacks on the tool semantic layer but also lays a solid foundation for developing corresponding defense mechanisms. Furthermore, it highlights the urgent need to establish trust and verification mechanisms within the agent-tool ecosystem.


