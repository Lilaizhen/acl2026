\section{Related Work}

\subsection{Tool-Augmented Agents}
Large Language Models (LLMs) have shifted from passive text generation to active problem-solving through the integration of external tools. ReAct~\cite{yao2023react} interleaves reasoning traces with tool execution, allowing models to maintain working memory and correct logical errors dynamically. Toolformer~\cite{schick2023toolformer} showed that LLMs can learn to invoke tools in a self-supervised manner, and Chameleon~\cite{lu2023chameleon} extended this to compositional reasoning by planning sequences of heterogeneous tools.

As tool ecosystems expand, efficient selection becomes a critical bottleneck. Benchmarks such as API-Bank~\cite{li2023apibank} and ToolBench~\cite{qin2023toolbench} standardize evaluation of an agent's ability to retrieve and call APIs. Typical pipelines use retriever-reader architectures with sparse (BM25) or dense (BERT-based) embeddings to filter candidates before an LLM selects a tool based on semantic similarity~\cite{qin2023toolbench}. These frameworks assume a benign tool supply chain, trusting that semantically relevant metadata correlates with safe functionality. Our work challenges this assumption by showing how this semantic dependency can be exploited.

\subsection{Adversarial Attacks on LLMs}
Adversarial attacks on LLMs span white-box and black-box settings. GCG (Greedy Coordinate Gradient)~\cite{zou2023universal} uses gradient-based token optimization to generate jailbreak suffixes that bypass safety guardrails. In black-box settings, methods such as PAIR~\cite{chao2023jailbreaking}, TAP (Tree of Attacks with Pruning)~\cite{mehrotra2023tree}, and AutoDAN~\cite{zhu2023autodan} rely on iterative prompt rewriting guided by an attacker model to improve attack success with limited queries.

For agents, threats progress from indirect prompt injection~\cite{greshake2023more}, where malicious instructions are embedded in retrieved content, to active tool selection attacks. Concurrent work such as ToolHijacker~\cite{shi2025promptinjectionattacktool}, ToolTweak~\cite{sneh2025tooltweakattacktoolselection}, and the Attractive Metadata Attack (AMA)~\cite{mo2025attractivemetadataattackinducing} independently optimizes tool metadata to hijack retrieval. These works emphasize the \textit{attraction} phase, maximizing selection probability. We differ by introducing a dedicated \textit{manipulation} phase that uses execution traces to adapt the malicious return payload, ensuring the agent is both attracted to and steered by the adversarial tool.
