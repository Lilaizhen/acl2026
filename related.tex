% \section{Related Work}

% \subsection{Tool-Augmented Agents}
% Large Language Models (LLMs) have shifted from passive text generation to active problem-solving through the integration of external tools. ReAct~\cite{yao2023react} interleaves reasoning traces with tool execution, allowing models to maintain working memory and correct logical errors dynamically. Toolformer~\cite{schick2023toolformer} showed that LLMs can learn to invoke tools in a self-supervised manner, and Chameleon~\cite{lu2023chameleon} extended this to compositional reasoning by planning sequences of heterogeneous tools.

% As tool ecosystems expand, efficient selection becomes a critical bottleneck. Benchmarks such as API-Bank~\cite{li2023apibank} and ToolBench~\cite{qin2023toolbench} standardize evaluation of an agent's ability to retrieve and call APIs. Typical pipelines use retriever-reader architectures with sparse (BM25) or dense (BERT-based) embeddings to filter candidates before an LLM selects a tool based on semantic similarity~\cite{qin2023toolbench}. These frameworks assume a benign tool supply chain, trusting that semantically relevant metadata correlates with safe functionality. Our work challenges this assumption by showing how this semantic dependency can be exploited.

% \subsection{Adversarial Attacks on LLMs}
% Adversarial attacks on LLMs span white-box and black-box settings. GCG (Greedy Coordinate Gradient)~\cite{zou2023universal} uses gradient-based token optimization to generate jailbreak suffixes that bypass safety guardrails. In black-box settings, methods such as PAIR~\cite{chao2023jailbreaking}, TAP (Tree of Attacks with Pruning)~\cite{mehrotra2023tree}, and AutoDAN~\cite{zhu2023autodan} rely on iterative prompt rewriting guided by an attacker model to improve attack success with limited queries.

% For agents, threats progress from indirect prompt injection~\cite{greshake2023more}, where malicious instructions are embedded in retrieved content, to active tool selection attacks. Concurrent work such as ToolHijacker~\cite{shi2025promptinjectionattacktool}, ToolTweak~\cite{sneh2025tooltweakattacktoolselection}, and the Attractive Metadata Attack (AMA)~\cite{mo2025attractivemetadataattackinducing} independently optimizes tool metadata to hijack retrieval. These works emphasize the \textit{attraction} phase, maximizing selection probability. We differ by introducing a dedicated \textit{manipulation} phase that uses execution traces to adapt the malicious return payload, ensuring the agent is both attracted to and steered by the adversarial tool.

\section{Related Work}

\subsection{Tool-Augmented Agents}
Large Language Models (LLMs) have evolved from passive text generation to autonomous problem-solving by leveraging external tools. ReAct~\cite{yao2023react} integrates reasoning traces with tool execution, enabling models to maintain working memory and dynamically correct reasoning errors. Toolformer~\cite{schick2024toolformer} demonstrated that LLMs can self-supervise tool invocation, while Chameleon~\cite{lu2023chameleon} extended this approach to compositional reasoning over heterogeneous tool sequences.

The expansion of tool ecosystems introduces a challenge in efficient tool selection. Benchmarks like API-Bank~\cite{li2023apibank} and ToolBench~\cite{qin2023toolbench} evaluate how well agents retrieve and invoke APIs. Typical pipelines employ retriever-reader architectures, using sparse (BM25) or dense (BERT-based) embeddings to filter candidates, followed by an LLM selecting a tool based on semantic similarity~\cite{qin2023toolbench}. These frameworks implicitly assume a benign tool supply chain, trusting that semantic metadata correlates with functional safety. In contrast, our work exposes how this semantic dependency can be exploited for adversarial purposes.

\subsection{Adversarial Attacks on LLMs}
Adversarial attacks on LLMs have been studied in both white-box and black-box settings. Methods like GCG (Greedy Coordinate Gradient)~\cite{zou2023universal} optimize token sequences to bypass safety mechanisms, while black-box approaches such as PAIR~\cite{chao2023jailbreaking}, TAP~\cite{mehrotra2023tree}, and AutoDAN~\cite{zhu2023autodan} iteratively rewrite prompts guided by an attacker model to increase success with limited queries.

For tool-augmented agents, attack surfaces expand beyond direct prompt manipulation. Indirect prompt injection~\cite{greshake2023more} embeds malicious instructions in retrieved content, whereas MCP-based systems face a new \emph{semantic supply-chain attack} surface. Concurrent works including ToolHijacker~\cite{shi2025toolhijacker}, ToolTweak~\cite{sneh2025tooltweak}, and the Attractive Metadata Attack (AMA)~\cite{mo2025attractive} focus on optimizing tool metadata to increase selection probability, corresponding to the \textit{attraction} phase. Our approach complements these by explicitly addressing the \textit{manipulation} phase: leveraging execution traces to iteratively refine the malicious payload, ensuring that selected tools can reliably hijack agent reasoning.

\subsection{Security Implications of Semantic Tool Attacks}
Prior studies on LLM security largely focus on user-to-agent attacks, including jailbreaks and prompt injections~\cite{zou2023universal,chao2023jailbreaking}. MCP-based agents introduce a fundamentally different risk: even when both the user and agent behave benignly, malicious third-party tools can hijack reasoning through trusted interfaces. This distinction motivates our two-stage Attraction-to-Manipulation (A2M) formulation, which systematically evaluates semantic supply-chain vulnerabilities and provides a structured framework for understanding tool-based adversarial strategies.
