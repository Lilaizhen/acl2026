\section{Related Work}

\subsection{Tool-Augmented Agents}
Large Language Models (LLMs)
have evolved from passive text generation
to autonomous problem-solving by leveraging external tools.
ReAct~\cite{yao2023reactsynergizingreasoningacting}
integrates reasoning traces with tool execution,
enabling models to maintain working memory
and dynamically correct reasoning errors.
Toolformer~\cite{schick2023toolformerlanguagemodelsteach}
demonstrated that LLMs can self-supervise tool invocation,
while Chameleon~\cite{lu2023chameleonplugandplaycompositionalreasoning}
extended this approach
to compositional reasoning
over heterogeneous tool sequences.

The expansion of tool ecosystems
introduces a challenge in efficient tool selection.
Benchmarks like API-Bank~\cite{li-etal-2023-api}
and ToolBench~\cite{qin2023toolllmfacilitatinglargelanguage}
evaluate how well agents retrieve and invoke APIs.
Typical pipelines employ retriever-reader architectures,
using sparse (BM25) or dense (BERT-based) embeddings
to filter candidates,
followed by an LLM selecting a tool
based on semantic similarity~\cite{qin2023toolllmfacilitatinglargelanguage}.
These frameworks implicitly assume a benign tool supply chain,
trusting that semantic metadata correlates with functional safety.
In contrast,
our work exposes how this semantic dependency
can be exploited for adversarial purposes.

\subsection{Adversarial Attacks on LLMs}
Adversarial attacks on LLMs have been studied
in both white-box and black-box settings.
Methods like Greedy Coordinate Gradient (GCG)~\cite{%
    zou2023universaltransferableadversarialattacks}
optimize token sequences to bypass safety mechanisms,
while black-box approaches
such as PAIR~\cite{chao2024jailbreakingblackboxlarge},
TAP~\cite{mehrotra2024treeattacksjailbreakingblackbox},
and AutoDAN~\cite{liu2024autodangeneratingstealthyjailbreak}
iteratively rewrite prompts guided by an attacker model
to increase success with limited queries.

For tool-augmented agents,
attack surfaces
expand beyond direct prompt manipulation.
Indirect prompt injection~\cite{greshake2023youvesignedforcompromising}
embeds malicious instructions in retrieved content,
whereas MCP-based systems
face a new \emph{semantic supply-chain attack} surface.
Concurrent works including ToolHijacker~\cite{shi2025promptinjectionattacktool},
ToolTweak~\cite{sneh2025tooltweakattacktoolselection},
and the Attractive Metadata Attack (AMA)~\cite{mo2025attractivemetadataattackinducing}
focus on optimizing tool metadata
to increase selection probability,
corresponding to the \textit{attraction} phase.
Our approach complements these
by explicitly addressing the \textit{manipulation} phase:
leveraging execution traces to iteratively refine the malicious payload,
ensuring that selected tools
can reliably hijack agent reasoning.

\subsection{Security Implications of Semantic Tool Attacks}
Prior studies on LLM security
largely focus on user-to-agent attacks,
including jailbreaks and prompt injections~\cite{%
    zou2023universaltransferableadversarialattacks,
    chao2024jailbreakingblackboxlarge}.
MCP-based agents
introduce a fundamentally different risk:
even when both the user and agent behave benignly,
malicious third-party tools
can hijack reasoning through trusted interfaces.
This distinction
motivates our two-stage Attraction-to-Manipulation (A2M) formulation,
which systematically evaluates semantic supply-chain vulnerabilities
and provides a structured framework
for understanding tool-based adversarial strategies.
