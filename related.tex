\section{Related work}

\subsection{AI Agents and External Tool Interaction Mechanisms}

The paradigm of enabling large language models to use external tools has emerged as a key path to enhancing their capabilities and grounding their responses in real-world data. Early works like \citet{schick2023toolformerlanguagemodelsteach} and \citet{qin2023tool} pioneered this direction, enabling LLMs to understand and call APIs through fine-tuning or instruction tuning. Subsequently, reasoning and acting frameworks such as ReAct \cite{yao2023reactsynergizingreasoningacting} and comprehensive platforms like AutoGPT \cite{yang2023autogptonlinedecisionmaking} and LangChain \cite{langchain2023} further advanced agent architectures by supporting the dynamic selection and execution of appropriate tools from a toolkit based on user queries. A \textbf{common cornerstone} of these methods is their heavy reliance on the natural language descriptions (metadata) of tools for semantic matching to decide which tool to invoke. However, prior studies \textbf{implicitly assume that all available tools originate from trusted sources}. In open agent ecosystems, this assumption no longer holds with the introduction of third-party MCP tools. External providers can register arbitrary tools whose metadata may be inaccurate or maliciously manipulated, thereby exposing a new and largely overlooked attack surface. Our work directly addresses this risk by systematically investigating security threats that arise when third-party MCP tool metadata is exploited for malicious purposes.


\subsection{Attacks Targeting AI Agents}

As AI agents become increasingly capable, their attack surfaces have correspondingly expanded, attracting extensive security research. A major line of work focuses on \textit{prompt injection attacks} \cite{yu2024assessingpromptinjectionrisks}, where adversaries craft malicious user inputs to hijack the model's instruction-following mechanism and cause unintended behaviors. Another important direction concerns \textit{privilege misuse} \cite{kim2025promptflowintegrity}, in which an agent, although properly authorized, is deceived through social engineering or logical flaws to perform unsafe operations such as deleting files or sending emails. More sophisticated threats include \textit{model jailbreaking} \cite{zhang2024instructionbackdoorattackscustomized} and \textit{data extraction attacks} \cite{shi2025promptinjectionattacktoolselection}.

While these studies have significantly deepened our understanding of agent security, the growing integration of third-party MCP tools introduces a new and largely unexplored attack vector. In open ecosystems, external providers can freely register MCP tools whose metadata (\texttt{name}, \texttt{desc}, and \texttt{out}) may not be trustworthy or verifiable. As agents rely on semantic matching over these natural-language descriptions to select tools, they can be misled into invoking malicious or manipulated tools registered by untrusted sources. This work systematically investigates this emerging threat and proposes an automated framework to generate and evaluate such metadata-level attacks within the MCP ecosystem.


\subsection{Automated Attack Generation Techniques}
The automation of adversarial example generation has been extensively explored in both computer vision and NLP domains. Traditional methods often rely on gradient-based approaches \cite{alzantot-etal-2018-generating} or genetic algorithms \cite{alzantot2019genattackpracticalblackboxattacks}. More recently, LLMs have been harnessed for this purpose, demonstrating remarkable capability in generating semantically meaningful adversarial examples \cite{paulus2025advprompterfastadaptiveadversarial}. Our work draws inspiration from these advances but applies them to the novel domain of tool metadata manipulation. Unlike prior work that focused on perturbing input data, we leverage a hybrid approach combining LLMs' creative generation capabilities with the structured search of genetic algorithms, further refined using semantic embeddings to ensure the generated attacks remain semantically coherent and stealthy while achieving their malicious objectives.
