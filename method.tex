\section{Threat Model}
\label{sec:threat_model}

We consider a security landscape within the Model Context Protocol (MCP) ecosystem where AI agents dynamically integrate tools from decentralized registries.

\subsection{Attack Surface: The Semantic Layer}
In contrast to traditional software vulnerabilities such as buffer overflows,
we identify a novel attack surface at the semantic layer of tool definitions.
An MCP tool is defined by a triplet $T = (\textit{name}, \textit{description}, \textit{schema})$.
Agents rely on Large Language Models (LLMs) to semantically match user queries to these definitions.
An attacker exploits this dependency by crafting adversarial metadata,
specifically the natural language description and output schema,
to hijack the agent's reasoning process without requiring the execution of malicious code on the host machine.

\subsection{Adversary Capabilities}
We assume a black-box adversary with specific capabilities.
The attacker can register a legitimate-looking tool in the MCP ecosystem.
They control the tool's name, description, and the textual content returned upon invocation ($\textit{out}$).
However, the attacker does not have access to the agent's internal model weights, system prompts, or private memory.
While the attacker cannot observe the agent's hidden states directly,
we assume in our framework that the attacker can simulate the victim agent locally to derive optimization signals from execution traces.

\subsection{Adversarial Objectives}
The goal of the adversary is to induce a specific target behavior $y_{\text{target}}$ by manipulating $T$.
We focus on four critical attack scenarios:
\begin{enumerate}
    \item \textbf{Cognitive Denial of Service (C-DoS).} The adversary constructs tool definitions that trap the agent in redundant reasoning loops or recursive tool invocations, exponentially inflating token consumption without contributing to task progress.
    \item \textbf{Contextual Information Exfiltration.} The adversary tricks the agent into retrieving sensitive data from its memory or local file system and passing it to the malicious tool's input arguments, thereby leaking private information.
    \item \textbf{Environment Integrity Compromise.} The adversary induces the agent to perform unauthorized write operations, such as modifying configuration files (e.g., \texttt{config.json}) or installing persistent backdoors, compromising the security state of the host environment.
    \item \textbf{Reasoning Derailment.} The adversary disrupts the agent's logical flow, causing it to hallucinate, select incorrect downstream tools, or terminate the session prematurely, resulting in task failure.
\end{enumerate}

\section{Method: SIGHT}
\label{sec:method}
This framework automates semantic tool hijacking by mimicking the cognitive process of a sophisticated attacker.
Unlike existing adversarial methods that rely on stochastic and blind optimization,
SIGHT starts with sociolinguistic persuasion priors and iteratively refines the attack by analyzing the victim agent's reasoning traces.

% [Optional: Insert Figure Here]
% \begin{figure}[t]
%     \centering
%     \includegraphics[width=\linewidth]{sight_framework.pdf}
%     \caption{Overview of the SIGHT framework.}
%     \label{fig:framework}
% \end{figure}

\subsection{Problem Formulation}
Let an agent be a function $\gA: (\gX, \gT) \rightarrow (\gY, \tau)$ that maps a user query $x \in \gX$ and a set of tools $\gT$ to a final response $y \in \gY$ and an execution trace $\tau$.
The trace $\tau = [s_0, a_0, \dots, s_k, a_k]$ represents the sequence of internal reasoning states and actions.
Our objective is to find an adversarial tool $T_{\text{adv}}$ that maximizes an attack utility function $\gF$.
The optimization problem is defined as follows:
\begin{equation}
    T^*_{\text{adv}} = \mathop{\arg\max}_{T \in \gT} \gF(\gA(x, \{T\}))
\end{equation}
Since $\gA$ is non-differentiable and the search space of natural language is discrete and sparse,
gradient-based optimization is infeasible.
We approximate the gradient using semantic feedback extracted from $\tau$.

\subsection{Strategic Initialization via Persuasion Priors}
Random initialization in the semantic space often yields incoherent tool definitions that are immediately filtered by the agent.
To address this, we inject domain knowledge via \textbf{Sociolinguistic Persuasion Priors}.
Drawing from Cialdini's principles of influence,
we partition the initialization manifold into four orthogonal dimensions $\gP = \{p_{\text{urg}}, p_{\text{auth}}, p_{\text{util}}, p_{\text{rel}}\}$:

\begin{itemize}
    \item \textbf{Urgency ($p_{\text{urg}}$):} Exploits the agent's bias towards time-sensitive tasks, such as generating a critical system alert.
    \item \textbf{Authority ($p_{\text{auth}}$):} Mimics trusted institutional protocols, such as a root verification suite.
    \item \textbf{Utility ($p_{\text{util}}$):} Overloads the definition with promises of comprehensive functionality, such as an all-in-one diagnostic tool.
    \item \textbf{Relevance ($p_{\text{rel}}$):} Maximizes semantic overlap with the query to trigger retrieval mechanisms.
\end{itemize}

We use a Generator LLM, denoted as $\gM_{\text{gen}}$, to sample the initial seed population $\gS_0$.
For each prior $p \in \gP$, we generate $k$ candidates.
The formulation is:
\begin{equation}
    \gS_0 = \bigcup_{p \in \gP} \{ T \mid T \sim \gM_{\text{gen}}(x, p) \}
\end{equation}
This ensures the optimization begins from a high-potential subspace.

\subsection{Trajectory-Aware Optimization}
We introduce a \textbf{Critic-Refiner} loop.
This mechanism treats the agent's trace $\tau$ as a natural language gradient to guide the search.

\paragraph{Trace Acquisition and Breakpoint Identification.}
In iteration $t$, we execute the agent with the current candidate $T_t$ and capture the trace $\tau_t$.
If the attack fails, the trace typically contains a \textbf{logical breakpoint}.
This is the specific reasoning step where the agent rejects the tool or deviates from the desired path.
For example, the trace might reveal that the agent considers the tool name valid but finds the description too vague regarding the file format.

\paragraph{Semantic Gradient Estimation.}
A Critic Model $\gM_{\text{critic}}$ analyzes the pair $(T_t, \tau_t)$ to compute the feedback $h_t$.
We prompt the critic to identify the cause of failure and propose a semantic modification:
\begin{equation}
    h_t = \gM_{\text{critic}}(T_t, \tau_t, y_{\text{target}})
\end{equation}
Here, $h_t$ acts as a directional vector in semantic space.
An example feedback would be an instruction to explicitly mention JSON schema support in the description.

\paragraph{Directed Mutation.}
A Refiner Model $\gM_{\text{refine}}$ applies this feedback to generate the next candidate:
\begin{equation}
    T_{t+1} = \gM_{\text{refine}}(T_t, h_t)
\end{equation}
This update rule $T_{t+1} \leftarrow T_t + h_t$ effectively performs gradient descent in the semantic manifold.
It iteratively aligns the tool definition with the agent's internal verification logic until the defense is bypassed.
