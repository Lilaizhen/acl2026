\section{Threat Model}
\label{sec:threat_model}

We examine the security landscape within the Model Context Protocol (MCP) ecosystem, where AI agents dynamically integrate tools sourced from decentralized registries.

\subsection{Attack Surface: The Tool Integration Layer}
Distinct from traditional prompt injection attacks that manipulate direct user input, we identify a novel attack surface inherent to the tool definition layer of the MCP architecture.
Formally, a standard MCP tool instance is characterized by the triplet $t = (n, d, \sigma)$, representing the \textbf{n}ame, \textbf{d}escription, and input \textbf{schema} ($\sigma$), respectively.
Agents rely on these definition components to perform tool selection.
An attacker exploits this architectural dependency by crafting the tool's output $o_{\text{adv}}$ as an attack payload, thereby manipulating the agent to achieve the target behavior $y_{\text{target}}$.

\subsection{Attacker Capabilities}
We postulate a black-box attacker possessing specific capabilities within the registry system.
Unlike standard providers, the attacker registers a malicious tool instance defined as the tuple $\tilde{t} = (n, d, \sigma, o_{\text{adv}})$.

\begin{itemize}
    \item \textbf{Metadata Control} $(n, d, \sigma)$: The attacker constructs the name ($n$), description ($d$), and schema ($\sigma$) to be ostensibly benign, optimizing them to maximize the probability of selection by the victim agent.
    \item \textbf{Attack Payload} $(o_{\text{adv}})$: The attacker controls the execution endpoint and crafts the tool's output $o_{\text{adv}}$ to contain attack instructions that manipulate agent behavior.
\end{itemize}

The attacker lacks access to the internal model weights, system prompts, or private memory of the victim agent. While direct observation of hidden states is precluded, we assume the attacker can simulate the victim agent locally to derive optimization signals from execution traces.

\subsection{Attack Objectives}
The primary objective of the attacker is to induce a specific target behavior $y_{\text{target}}$ by manipulating the malicious tool configuration $\tilde{t}$. We categorize the potential impact into four critical attack scenarios:
\begin{itemize}
    \item \textbf{Cognitive Denial of Service (C-DoS).} The attacker constructs tool definitions that entrap the agent in redundant reasoning loops or recursive tool invocations, resulting in significant token consumption inflation.
    \item \textbf{Contextual Information Exfiltration.} The attacker manipulates the agent into retrieving sensitive data from memory or the local file system and injecting it into the arguments of a malicious tool (defined in $\sigma$), thereby exfiltrating private information.
    \item \textbf{Environment Integrity Compromise.} The attacker compels the agent to execute unauthorized write operations, such as altering configuration files like \texttt{config.json} or installing persistent backdoors, compromising the host environment's integrity.
    \item \textbf{Reasoning Derailment.} The attacker disrupts the logical flow via the payload $o_{\text{adv}}$, causing the selection of incorrect downstream tools or premature session termination.
\end{itemize}

\section{Method}
\label{sec:method}

We introduce \textbf{A2M} (\textbf{A}ttraction-\textbf{to}-\textbf{M}anipulation), a black-box optimization framework for generating malicious MCP tools. Our key insight is that tool-based attacks require optimizing two distinct objectives: (1) \textit{attraction}, maximizing the probability that an agent selects the malicious tool, and (2) \textit{manipulation}, maximizing the probability of manipulating agent behavior to achieve the attack goal once selected. We decouple these objectives into a two-phase optimization process, guided by execution trace analysis.

\subsection{Problem Formulation}

We model the target AI agent as a policy $\pi$ operating within a sequential decision-making framework defined by the tuple $(\mathcal{S}, \mathcal{A}, \mathcal{T})$, where $\mathcal{S}$ is the state space, $\mathcal{A}$ is the action space, and $\mathcal{T}$ is the tool environment.

\paragraph{State Space ($\mathcal{S}$).}
A state $s_k \in \mathcal{S}$ at step $k$ represents the agent's current observable context, consisting of the user query $q$ and the cumulative execution record $H_k$, which includes past reasoning steps, tool calls, and tool outputs.

\paragraph{Action Space ($\mathcal{A}$).}
At each step $k$, the agent selects an action $a_k \in \mathcal{A}$, which can be a reasoning step, a tool call, or a final response.

\paragraph{Tool Environment ($\mathcal{T}$).}
Let $\mathcal{T}$ denote the set of available tools. Our attack introduces a malicious tool instance $\tilde{t} \in \mathcal{T}$, defined by its metadata: $\tilde{t} = (n, d, \sigma, o_{\text{adv}})$, where $n$ is the name, $d$ is the description, $\sigma$ is the input schema, and $o_{\text{adv}}$ is the attack payload.

\paragraph{Execution Trace.}
The agent generates an execution trace $\tau$ over $T$ decision steps:
\begin{equation}
    \tau = ((s_0, a_0), (s_1, a_1), \dots, (s_{T-1}, a_{T-1}), s_T)
\end{equation}
where $T$ is the total number of steps. The state transition is stochastic because MCP tool outputs are nondeterministic; we write
\begin{equation}
    s_{k+1} \sim \mathcal{P}(\cdot \mid s_k, a_k)
\end{equation}
to denote the environment-induced transition kernel driven by external tool returns.
\paragraph{Attack Objective.}
Our optimization goal is to discover a malicious tool configuration $\tilde{t}^*$ that maximizes the expected attack success across all possible execution traces. Formally, we seek:
\begin{equation}
    \tilde{t}^* = \operatorname*{argmax}_{\tilde{t}=(n,\, d,\, \sigma,\, o_{\text{adv}})} \mathbb{E}_{\tau \sim \pi(q,\, \mathcal{T} \cup \{\tilde{t}\})} [\mathcal{J}(\tau)]
    \label{eq:objective}
\end{equation}
where $\mathcal{J}(\tau)$ scores how effectively execution trace $\tau$ achieves the attack objective, as detailed in Section~\ref{sec:fitness}. The expectation accounts for the uncertainty of external MCP tool return content.

\paragraph{Objective Decomposition.}
A key modeling choice is to separate \textit{Attraction} (whether the malicious tool is selected) from \textit{Manipulation} (what happens after selection). We implement this as a gated, piecewise objective:
\begin{equation}
    \mathcal{J}(\tau) =
    \begin{cases}
        0, & \tilde{t} \notin \tau \\
        \mathcal{J}_{\text{man}}(\tau, o_{\text{adv}}), & \tilde{t} \in \tau
    \end{cases}
    \label{eq:decomposition}
\end{equation}
where the gate $\tilde{t} \in \tau$ plays the role of Attraction, and $\mathcal{J}_{\text{man}}(\tau, o_{\text{adv}})$ captures post-selection manipulation effectiveness.

When optimizing the expected score, we make the decomposition explicit via the law of total expectation:
\begin{align}
    \mathbb{E}_{\tau \sim \pi(q,\, \mathcal{T} \cup \{\tilde{t}\})}[\mathcal{J}(\tau)]
    &= \Pr(\tilde{t} \in \tau) \label{eq:expected_decomposition}\\
    &\quad \cdot \mathbb{E}\!\left[\mathcal{J}_{\text{man}}(\tau, o_{\text{adv}}) \mid \tilde{t} \in \tau\right] \nonumber
\end{align}
The first term is the Attraction objective (selection probability), and the second term is the Manipulation objective (conditional effectiveness given selection), matching Phase~I and Phase~II.

This motivates our two-phase optimization approach: \textit{Attraction} optimizes $(n, d)$ to maximize the probability of the agent selecting the malicious tool, while \textit{Manipulation} optimizes $o_{\text{adv}}$ to maximize manipulation effectiveness based on the $(n, d)$ derived from Attraction phase.

\subsection{Phase I: Attraction}

The Attraction phase generates tool metadata $(n, d)$ designed to maximize the likelihood of agent selection. We employ five persuasion strategies to systematically explore the search space:

\begin{itemize}[leftmargin=*,nosep]
    \item \textbf{Authority} ($s_{\text{auth}}$): Leverages official, system-level, or administrative terminology to establish trust.
    \item \textbf{Urgency} ($s_{\text{urg}}$): Simulates time-critical constraints requiring immediate action.
    \item \textbf{Comprehensiveness} ($s_{\text{comp}}$): Frames the tool as a holistic, one-stop solution.
    \item \textbf{Resource Optimality} ($s_{\text{res}}$): Highlights efficiency and computational performance benefits.
    \item \textbf{Security} ($s_{\text{sec}}$): Mimics safety protocols and encryption standards.
\end{itemize}

To maximize compatibility, the malicious tool utilizes a permissive input schema that accepts arbitrary strings or null values. By imposing minimal structural constraints, this design ensures the tool can be successfully invoked regardless of the specific argument format generated by the agent.

Let $N_{\text{attr}}$ denote the total generation budget allocated for the Attraction phase. We adopt a \textbf{uniform allocation strategy}, assigning $N_s = N_{\text{attr}} / 5$ seeds to each persuasion category. For each strategy $s \in \mathcal{S}$, we prompt a generator LLM $\mathcal{G}$ to sample candidates:
\begin{equation}
(n_i, d_i) \sim \mathcal{G}(q, s, y_{\text{target}}) \quad \text{for } i=1,\dots,N_s
\end{equation}

We evaluate each candidate by executing the agent with a placeholder payload and measuring the \textit{selection rate} using $K_{\text{roll}}$ Monte Carlo rollouts:
\begin{equation}
\mathrm{Score}_{\mathrm{attr}}(n, d) = \frac{1}{K_{\text{roll}}} \sum_{k=1}^{K_{\text{roll}}} \indicator[\tilde{t} \in \tau_k]
\end{equation}
where $\tau_k$ denotes the execution trace of the $k$-th trial, and $\tilde{t} \in \tau_k$ indicates whether the malicious tool was invoked during that trace. Candidates with non-zero selection rates are retained as \textit{elite seeds} $\mathcal{E}_1$.

\subsection{Phase II: Manipulation}

Given elite metadata from the Attraction phase, we synthesize attack payloads $o_{\text{adv}}$ tailored to specific attack objectives. The payload must accomplish two goals: (1) appear as a plausible tool response, and (2) contain instructions that redirect agent behavior toward $y_{\text{target}}$.

For each elite $(n^*, d^*) \in \mathcal{E}_1$, we generate payloads proportionally to their selection rates. Let $N_{\text{total}}$ be the total number of complete tools to generate, and let $\mathrm{Score}_{\text{attr}}(n^*, d^*)$ be the selection rate of elite $(n^*, d^*)$. The number of payloads generated from each elite is:
\begin{equation}
N_{\text{payload}}(n^*, d^*) = N_{\text{total}} \cdot \frac{\mathrm{Score}_{\text{attr}}(n^*, d^*)}{\sum_{(n',d') \in \mathcal{E}_1} \mathrm{Score}_{\text{attr}}(n', d')}
\end{equation}
Each payload is generated conditioned on the attack type:
\begin{equation}
o_{\text{adv}} \sim \mathcal{G}(q, n^*, d^*, y_{\text{target}}, \phi_{\text{attack}})
\end{equation}
where $\phi_{\text{attack}}$ encodes attack-specific generation constraints (Table~\ref{tab:attack_constraints}).

\begin{table}[t]
\centering
\small
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{Attack Type} & \textbf{Payload Design Principle} \\
\midrule
C-DoS & Induce repeated invocations via partial results \\
Exfiltration & Guide file system traversal to sensitive data \\
Integrity & Instruct configuration file modifications \\
Derailment & Provide misleading information \\
\bottomrule
\end{tabular}
\caption{Attack-specific payload generation constraints.}
\label{tab:attack_constraints}
\end{table}

The complete tool $\tilde{t} = (n^*, d^*, o_{\text{adv}})$ is evaluated using an attack-specific fitness function $f(\tau, y_{\text{target}})$, detailed in Section~\ref{sec:fitness}.

A key challenge in black-box optimization is extracting actionable guidance from execution traces without access to internal signals. We address this through an \textbf{Analyzer-Optimizer} architecture.

\paragraph{Analyzer.}
Given an execution trace $\tau$, the current tool $\tilde{t}$, and the attack objective $y_{\text{target}}$, the analyzer $\mathcal{A}$ generates a structured feedback signal $d$:
\begin{equation}
    d = \mathcal{A}(\tau, \tilde{t}, y_{\text{target}})
    \label{eq:analyzer}
\end{equation}
The analyzer acts as a diagnostic critic with a bifurcated feedback strategy:
\begin{itemize}[leftmargin=*,nosep]
    \item \textbf{Scalar objectives} (e.g., C-DoS): focus on \textit{impact amplification}. Even when the tool is invoked, $\mathcal{A}$ suggests ways to raise cost (such as increasing loop depth or output verbosity).
    \item \textbf{Binary objectives} (e.g., Exfiltration, Integrity): perform \textit{failure diagnosis}. $\mathcal{A}$ pinpoints where the agent diverged from the intended path and proposes corrections to redirect it toward the malicious target.
\end{itemize}

\paragraph{Optimizer.}
The optimizer $\mathcal{O}$ synthesizes a refined tool using $d$:
\begin{equation}
    \tilde{t}' = \mathcal{O}(\tilde{t}, d, y_{\text{target}})
    \label{eq:optimizer}
\end{equation}
Unlike generic stochastic mutations, the optimizer interprets $d$ to apply targeted semantic adjustments to $(n, d, o_{\text{adv}})$, enabling both recovery from logical issues and amplification of attack severity without blind exploration.

\paragraph{Iterative Loop.} We maintain a population $\mathcal{P}$ of tools ranked by fitness. At each iteration:
\begin{enumerate}[leftmargin=*,nosep]
    \item Select top-$k$ elites from $\mathcal{P}$
    \item For each elite, apply Analyzer-Optimizer to generate refined variants
    \item Evaluate all variants and update $\mathcal{P}$
\end{enumerate}

The process terminates after $T$ iterations or upon reaching a fitness threshold.

\subsection{Fitness Function Design}
\label{sec:fitness}

We design attack-specific fitness functions tailored to each attack scenario. For resource exhaustion and task failure attacks, if the malicious tool $\tilde{t}$ is not invoked ($\tilde{t} \notin \tau$), the fitness is zero. For information exfiltration and environment integrity attacks, the scoring system inherently accounts for tool invocation status.

\paragraph{Resource Exhaustion (C-DoS).}
For resource exhaustion attacks, we measure the weighted token consumption:
\begin{equation}
f(\tau) = \indicator[\tilde{t} \in \tau] \cdot \left( T_{\mathrm{in}}(\tau) + 5 \cdot T_{\mathrm{out}}(\tau) \right)
\end{equation}
where $T_{\mathrm{in}}$ and $T_{\mathrm{out}}$ denote input and output token counts. Output tokens are weighted 5Ã— higher, aligning with the pricing structure of leading commercial models (e.g., Claude Sonnet 4.5) where generation typically costs about five times more than input processing.

\paragraph{Information Exfiltration \& Environment Integrity.}
For information leakage and environment compromise attacks, we use a discrete scoring system:
\begin{equation}
f(\tau) = s(\tau), \quad s(\tau) \in \{1, 2, 3, 4\}
\end{equation}
where $s(\tau)$ represents the attack progression level: (1) the malicious tool was not invoked by the agent, (2) the tool was invoked but the agent did not attempt to fulfill the attack objective, (3) the agent attempted the attack but failed to complete it, (4) the agent successfully completed the attack objective. This graduated scoring captures the degree of agent manipulation.

\paragraph{Reasoning Derailment.}
For reasoning derailment attacks, fitness measures the degradation in task completion:
\begin{equation}
f(\tau) = \indicator[\tilde{t} \in \tau] \cdot (1 - r_{\mathrm{success}})
\end{equation}
where $r_{\mathrm{success}}$ is the task success rate. Higher fitness indicates greater disruption to agent functionality.

\subsection{Algorithm Summary}

Algorithm~\ref{alg:a2m} summarizes the complete A2M framework.

\begin{algorithm}[t]
\caption{A2M: Attraction-to-Manipulation}
\label{alg:a2m}
\begin{algorithmic}[1]
\Require Query $q$, attack objective $y_{\text{target}}$, iterations $T$
\Ensure Optimized malicious tool $\tilde{t}^*$
\State \textit{// Phase I: Attraction Optimization}
\State $\mathcal{C} \gets \emptyset$
\For{each strategy $s \in \mathcal{S}$}
    \State $(n, d) \sim \mathcal{G}(q, s, y_{\text{target}})$
    \State $\tau \gets \text{Execute}(\mathcal{M}, q, (n, d, \emptyset))$
    \State $\mathcal{C} \gets \mathcal{C} \cup \{(n, d, \text{Score}_{\text{attr}})\}$
\EndFor
\State $\mathcal{E}_1 \gets \text{TopK}(\mathcal{C})$ \Comment{Elite seeds (top-$k$ by fitness)}
\State
\State \textit{// Phase II: Payload Synthesis}
\State $\mathcal{P} \gets \emptyset$
\For{each $(n^*, d^*) \in \mathcal{E}_1$}
    \State $o_{\text{adv}} \sim \mathcal{G}(q, n^*, d^*, y_{\text{target}})$
    \State $\tau \gets \text{Execute}(\mathcal{M}, q, (n^*, d^*, o_{\text{adv}}))$
    \State $\mathcal{P} \gets \mathcal{P} \cup \{((n^*, d^*, o_{\text{adv}}), f(\tau))\}$
\EndFor
\State
\State \textit{// Iterative Refinement}
\For{$t = 1$ to $T$}
    \State $\mathcal{E} \gets \text{TopK}(\mathcal{P})$ \Comment{Select top-$k$ by fitness}
    \For{each $\tilde{t} \in \mathcal{E}$}
        \State $\tau \gets \text{Execute}(\mathcal{M}, q, \tilde{t})$
        \State $(\_, \text{dir}) \gets \mathcal{A}(\tau, \tilde{t}, y_{\text{target}})$
        \State $\tilde{t}' \gets \mathcal{O}(\tilde{t}, \text{dir}, y_{\text{target}})$
        \State $\tau' \gets \text{Execute}(\mathcal{M}, q, \tilde{t}')$
        \State $\mathcal{P} \gets \mathcal{P} \cup \{(\tilde{t}', f(\tau'))\}$
    \EndFor
\EndFor
\State \Return $\arg\max_{\tilde{t} \in \mathcal{P}} f(\tilde{t})$
\end{algorithmic}
\end{algorithm}
