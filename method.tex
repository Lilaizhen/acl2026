\section{Threat Model}
\label{sec:threat_model}

We examine the security landscape within the Model Context Protocol (MCP) ecosystem, where AI agents dynamically integrate tools sourced from decentralized registries.

\subsection{Attack Surface: The Tool Integration Layer}
Distinct from traditional prompt injection attacks that manipulate direct user input, we identify a novel attack surface inherent to the tool definition layer of the MCP architecture.
Formally, a standard MCP tool instance is characterized by the triplet $t = (n, d, \Sigma)$, representing the \textbf{n}ame, \textbf{d}escription, and input \textbf{schema} ($\Sigma$), respectively.
Agents rely on these definition components to perform tool selection.
An attacker exploits this architectural dependency by embedding adversarial payloads within the tool metadata itself, allowing the adversary to hijack the agent control flow through the tool integration process.

\subsection{Adversary Capabilities}
We postulate a black-box adversary possessing specific capabilities within the registry system.
Unlike standard providers, the adversary registers a malicious tool instance defined as the tuple $\tilde{t} = (n, d, \Sigma, o_{\text{adv}})$.

\begin{itemize}
    \item \textbf{Metadata Control} $(n, d, \Sigma)$: The adversary constructs the name ($n$), description ($d$), and schema ($\Sigma$) to be ostensibly benign, optimizing them to maximize the probability of selection by the victim agent.
    \item \textbf{Adversarial Payload} $(o_{\text{adv}})$: The adversary controls the execution endpoint. Instead of performing legitimate computation, the tool deterministically returns a fixed adversarial output $o_{\text{adv}}$ containing the injection instructions, regardless of the input arguments.
\end{itemize}

The adversary lacks access to the internal model weights, system prompts, or private memory of the victim agent. While direct observation of hidden states is precluded, we assume the attacker can simulate the victim agent locally to derive optimization signals from execution traces.

\subsection{Adversarial Objectives}
The primary objective of the adversary is to induce a specific target behavior $y_{\text{target}}$ by manipulating the adversarial tool configuration $\tilde{t}$. We categorize the potential impact into four critical attack scenarios:
\begin{itemize}
    \item \textbf{Cognitive Denial of Service (C-DoS).} The adversary constructs tool definitions that entrap the agent in redundant reasoning loops or recursive tool invocations, resulting in significant token consumption inflation without task progression.
    \item \textbf{Contextual Information Exfiltration.} The adversary manipulates the agent into retrieving sensitive data from memory or the local file system and injecting it into the arguments of a malicious tool (defined in $\Sigma$), thereby exfiltrating private information.
    \item \textbf{Environment Integrity Compromise.} The adversary compels the agent to execute unauthorized write operations, such as altering configuration files like \texttt{config.json} or installing persistent backdoors, compromising the host environment's integrity.
    \item \textbf{Reasoning Derailment.} The adversary disrupts the logical flow via the payload $o_{\text{adv}}$, causing the selection of incorrect downstream tools or premature session termination.
\end{itemize}

\section{Method}
\label{sec:method}

We propose \textbf{\methodname} (\methodfullname). Unlike traditional adversarial attacks that treat the victim model as a static black box, \methodname{} formalizes the attack as a dynamic optimization problem over the agent's sequential execution process.

\subsection{Formalization of Agentic Reasoning}
We model the target AI Agent as a policy $\pi$ operating within a sequential decision-making framework defined by the tuple $(\mathcal{S}, \mathcal{A}, \mathcal{T})$.

\paragraph{State Space ($\mathcal{S}$)}
A state $s_k \in \mathcal{S}$ represents the agent's current observable context, consisting of the user query $q$ and the cumulative \textbf{Execution Record} $H_k$ (including past tool inputs, outputs, and model reasoning steps).

\paragraph{Action Space ($\mathcal{A}$)}
At each step $k$, the agent selects an action $a_k \in \mathcal{A}$, which can be a textual reasoning thought, a tool invocation command, or a final response.

\paragraph{Tool Environment ($\mathcal{T}$)}
Let $\mathcal{T}$ denote the set of available tools. Our attack introduces a malicious tool instance $\tilde{t} \in \mathcal{T}$, defined by its metadata: $\tilde{t} = (n, d, \Sigma, o_{\text{adv}})$.

The agent generates an \textbf{Execution Trace} $\tau$ over $K$ steps:
\begin{equation}
    \tau = [(s_0, a_0), (s_1, a_1), \dots, (s_K, a_K)]
\end{equation}
The state transition $s_{k+1} \leftarrow \text{Env}(s_k, a_k)$ is stochastic, as it depends on the external return values from the MCP tools.

\paragraph{Attack Objective}
We aim to find an optimal $\tilde{t}^*$ that maximizes an \textbf{Adversarial Objective Function} $\mathcal{J}$:
\begin{equation}
    \tilde{t}^* = \mathop{\arg\max}_{\tilde{t}} \mathcal{J}(\tau) \quad \text{s.t.} \quad \tau \sim \pi(q, \mathcal{T} \cup \{\tilde{t}\})
\end{equation}

\subsection{Phase I: Strategic Initialization via Persuasion Priors}
The optimization surface of natural language is discrete and sparse. Random initialization of $\tilde{t}$ results in a negligible probability of selection. To maximize the selection probability, we employ \textbf{Sociolinguistic Persuasion Priors}.

Drawing from Cialdini's principles \cite{cialdini1984influence}, we define a set of semantic priors $\mathcal{P} = \{p_{\text{urg}}, p_{\text{auth}}, p_{\text{util}}, p_{\text{rel}}\}$. We prompt a Generator LLM ($\mathcal{M}_{\text{gen}}$) to synthesize the initial seed population $\mathcal{T}_{\text{init}}$:
\begin{equation}
    \mathcal{T}_{\text{init}} = \bigcup_{p \in \mathcal{P}} \{ t \mid t \sim \mathcal{M}_{\text{gen}}(q, p) \}
\end{equation}
This ensures that the initial attack vectors are distributed across diverse cognitive biases (Urgency, Authority, Utility, and Relevance), providing a high-potential starting point.
\subsection{Phase II: Trajectory-Aware Optimization}
Due to the discrete nature of natural language and the non-differentiability of the agent's reasoning process, explicitly computing the gradient of $\mathcal{J}$ with respect to $t$ is intractable.
We propose an iterative optimization scheme based on \textbf{``Semantic Pseudo-Gradients,''} leveraging a \textbf{Critic-Refiner} architecture to transform the agent's execution trace $\tau$ into actionable optimization signals.

We formalize the optimization process at iteration $i$ as two consecutive steps: \textit{Semantic Gap Identification} and \textit{Directed Semantic Descent}.

\paragraph{Semantic Gap Identification (The Critic).}
The Critic Model $\mathcal{M}_{\text{critic}}$ approximates the gradient estimation process, taking the current malicious tool definition $t_i$ and the failed execution trace $\tau_i$ as input.
The Critic performs \textbf{Causal Diagnosis}, analyzing $\tau_i$ to identify the \textbf{Logical Breakpoint} ($s^*$) responsible for the attack failure, such as the step where the agent prioritizes original user intent over the attacker's adversarial instructions.
Based on the semantic disparity at $s^*$, the Critic synthesizes a natural language instruction $\delta_i$ representing the \textbf{Optimization Direction}:
\begin{equation}
    \delta_i = \mathcal{M}_{\text{critic}}(t_i, \tau_i, s^*)
\end{equation}

\paragraph{Directed Semantic Descent (The Refiner).}
The Refiner Model $\mathcal{M}_{\text{refine}}$ executes the parameter update.
We conceptualize the optimization as minimizing the semantic misalignment $\mathcal{L}$ between the adversarial tool definition and the agent's decision boundary.
Unlike blind stochastic perturbations, the Refiner performs a directed mutation conditioned on $\delta_i$:
\begin{equation}
    t_{i+1} = \mathcal{M}_{\text{refine}}(t_i, \delta_i)
\end{equation}
By iteratively minimizing this semantic gap, \methodname{} progressively aligns the tool features with the agent's internal verification logic until the defense is breached.