\section{Method}

\subsection{Key Observations and Insights}

The definition space of MCP tools is inherently open-ended, where each tool is represented by a \texttt{name}, \texttt{desc}, and \texttt{out} field expressed in natural language. While this design grants agents remarkable flexibility and extensibility, it simultaneously introduces new security vulnerabilities. An attacker can exploit this openness by manipulating tool metadata to influence an agent's tool selection and internal reasoning process. Our analysis shows that even a single malicious tool can significantly compromise agent behavior, resulting in excessive resource consumption, task failure, or unintended information disclosure.

We further observe that adversarial effectiveness arises from a balance between \emph{semantic alignment} and \emph{semantic diversity}. High alignment with the agent's decision heuristics ensures that the candidate tool is likely to be selected, while diversity across semantic dimensions allows the discovery of qualitatively different and previously unseen attack vectors. Pure local optimization tends to converge to homogeneous variants with limited impact, whereas broad semantic exploration enables the uncovering of stronger, more generalizable attacks. Effective adversarial construction thus requires maintaining this dual emphasis on exploitation and exploration within the tool-definition space.

These insights motivate the design of our \textbf{Semantic MCP Tool Hijacking (SMTH)} framework, which strategically combines exploitation of high-fitness candidates with semantic diversity-driven exploration. This approach allows systematic discovery of robust adversarial tools and provides a deeper understanding of the latent vulnerabilities in MCP-powered agents.

\subsection{Threat Model}
\label{sec:threat-model}
We consider a threat model where the adversary interacts with an agent through the Model Context Protocol (MCP).

\paragraph{Attack's Potential Scenarios.}
MCP-powered agents typically act as intermediaries between user queries and external data sources. For example, conversational assistants may retrieve information from webpages or documents, code assistants may access project repositories, and automated workflow agents may invoke third-party APIs. These systems often allow the integration of externally hosted MCP tools, as long as the tool declares its interface and functional description.

In such scenarios, users benefit from convenience access, while the application itself bears the computational cost of reasoning and execution. An adversary can register a seemingly non-attack MCP tool that returns carefully crafted content or data, thereby influencing the agent's internal reasoning process. Because these returned contents are consumed internally by the model and not directly displayed to the user, such manipulations are difficult to detect.

\paragraph{Adversary's Target.}
The adversary's immediate target is the agent-tool interaction layer rather than the end-user interface. The goal is not primarily to alter the final rendered text seen by the user, but to perturb the agent's internal decision logic. By influencing which tools the agent selects and how it invokes them, an attacker can induce excessive resource consumption, steer the reasoning process toward incorrect or suboptimal outcomes, or establish persistent interference that affects subsequent sessions or tasks.

\paragraph{Formalization.}
Each MCP tool is represented as a triplet
\begin{equation}
T = (\texttt{name},\ \texttt{desc},\ \texttt{out}),
\end{equation}
where \texttt{name} is the tool identifier, \texttt{desc} is a natural-language description of the tool's functionality, and \texttt{out} is the content returned upon invocation, either structured data or free-form text. An agent \(A\) operates in a context \(C\) (user query, dialogue history, etc.) and produces a tool-call sequence
\begin{equation}
\tau = (T_{i_1}, T_{i_2}, \ldots),
\end{equation}
updating its internal state based on returned outputs and conditioning subsequent calls on prior results.

\paragraph{Adversary's Objectives.}
We define four primary adversarial objectives, each corresponding to an evaluation scenario used in our experiments:
\begin{enumerate}
  \item \textbf{Resource exhaustion.} Induce redundant or repeated reasoning and or tool calls without reducing the original task completion rate.
  \item \textbf{Task failure.} Cause the agent to fail at the user's intended task, for example produce incorrect results, return incomplete answers, loop, or time out. We quantify this as the reduction in task success rate compared to baseline.
  \item \textbf{Backdoor injection.} Induce the agent to persistently modify MCP configuration or register attacker-controlled parameters.
  \item \textbf{Information leakage.} Induce the agent to disclose sensitive information such as API keys, private dialogue history, or confidential fields.
\end{enumerate}

\paragraph{Adversary's Capabilities.}
We assume a realistic but bounded adversary: the attacker controls a single third-party MCP endpoint and may register one malicious tool \(T_{\text{adv}}=(\texttt{name},\ \texttt{desc},\ \texttt{out})\) before an evaluation run. During each run the tool definition remains fixed, non-adaptive and one-shot black-box, and the attacker has no access to the agent's internal state, model weights, filesystem, or communication channels. Their influence is limited to the responses served by the controlled MCP tool.

\subsection{Attack Method: Semantic MCP Tool Hijacking (SMTH)}
\label{sec:attack-method}
We frame the construction of adversarial MCP tools as a \emph{semantic search} problem over the tool-definition space \(T=(\texttt{name},\texttt{desc},\texttt{out})\): given a task context \(u\), find \(T\) that maximizes an attacker-centric fitness \(F(T)\). In light of the open and complex nature of the tool definition space, we adopt an automated evolutionary search strategy to generate adversarial tools.

\paragraph{Method overview.}
We use an LLM to generate \(m\) initial candidate tools conditioned on the task context \(u\). Each candidate is evaluated by a scenario-specific fitness \(F(\cdot)\) and the population is ranked. The current best candidate \(T^*\) is defined as the one with the highest fitness. To encourage exploration while preserving strong solutions, the top \(k\) candidates are projected into a semantic vector space, and from them we select the candidate \(T_j\) that is \emph{most semantically distant} from \(T^*\). The semantic crossover module \(\mathcal{C}(T^*,T_j)\) generates semantically coherent offspring by merging the semantic information of the two parents. Offspring are scored and incorporated into the population; this scoring and crossover loop is repeated for \(n\) iterations. Finally, the single highest-fitness candidate \(T_{\text{adv}}^*\) is selected as the adversarial tool for evaluation under the threat model.

Figure~\ref{fig:timeline-threat-model} illustrates our threat model through a timeline-based interaction flow among users, agents, and MCP servers.

\paragraph{Fitness.}
For the resource-exhaustion scenario, \(F(T)\) is the total number of tokens consumed by the agent when interacting with \(T\). In malicious-task scenarios, \(F(T)\) is defined as an automated score that measures the agent's degree of completion of the attack objective.

\paragraph{Semantic crossover.}
At iteration \(t\), let \(\mathcal{P}_{t-1}\) denote the current population. Compute \(F(T)\) for all \(T\in\mathcal{P}_{t-1}\), let \(\mathcal{S}\) be the top-\(k\) subset of \(\mathcal{P}_{t-1}\), and obtain embeddings \(e_T=\mathrm{Embed}(T)\) for each \(T\in\mathcal{S}\). Define
\begin{align}
T^*&=\arg\max_{T\in\mathcal{S}}F(T), \\
T_j&=\arg\max_{T\in\mathcal{S}\setminus\{T^*\}}\mathrm{dist}(e_T,e_{T^{*}}).
\end{align}
The operator \(\mathcal{C}(T^*,T_j)\) employs an LLM to semantically merge the two parent tools, producing offspring that integrate their key semantic content while preserving overall length and structural coherence. The offspring are inserted into the population, and the loop continues for \(n\) rounds. This "farthest-selection" strategy explicitly promotes semantic diversity and helps the search escape local optima while maintaining exploitation of high-fitness candidates.

\paragraph{Output and reproducibility.}
After \(n\) iterations we select the single best candidate:
\begin{equation}
T_{\text{adv}}^*=\arg\max_{T\in\mathcal{P}_n}F(T),
\end{equation}
and use \(T_{\text{adv}}^*\) to evaluate the target agent's robustness. In our implementation we set \(m=10\), \(k=5\), and \(n=20\). 
\begin{algorithm}[t]
\caption{SMTH: Semantic MCP Tool Hijacking}
\label{alg:smth-crossover}
\begin{algorithmic}[1]
\Require candidate generator (LLM in our implementation), task context \(u\), seed size \(m\), top \(k\), iterations \(n\), fitness \(F(\cdot)\), embedding \(\mathrm{Embed}(\cdot)\), distance \(\mathrm{dist}(\cdot,\cdot)\), crossover \(\mathcal{C}(\cdot,\cdot)\)
\Ensure Best adversarial tool \(T_{\text{adv}}^{*}\)
\State \(\mathcal{P}_0 \leftarrow \{\text{InitGen}_i(u)\}_{i=1}^m\) \Comment{initial population}
\For{\(t \gets 1\) to \(n\)}
  \State Compute \(F(T)\) for each \(T \in \mathcal{P}_{t-1}\)
  \State \(\mathcal{S} \leftarrow \text{top-}k(\mathcal{P}_{t-1}, F)\)
  \State \(T^* \leftarrow \arg\max_{T\in\mathcal{S}} F(T)\)
  \State Compute \(e_T \leftarrow \mathrm{Embed}(T)\) for each \(T\in\mathcal{S}\)
  \State \(T_j \leftarrow \arg\max_{T\in(\mathcal{S}\setminus\{T^*\})} \mathrm{dist}(e_T, e_{T^*})\)
  \State \(o_t \leftarrow \mathcal{C}(T^*, T_j)\) \Comment{semantic recombination and LLM validity check}
  \State \(\mathcal{P}_t \leftarrow \mathcal{P}_{t-1} \cup \{o_t\}\)
  \State Optionally prune or re-rank \(\mathcal{P}_t\) to bound population size
\EndFor
\State \(T_{\text{adv}}^{*} \leftarrow \arg\max_{T\in\mathcal{P}_n} F(T)\)
\State \Return \(T_{\text{adv}}^{*}\)
\end{algorithmic}
\end{algorithm}
