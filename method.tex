\section{Threat Model}
\label{sec:threat_model}

We examine the security landscape within the Model Context Protocol (MCP) ecosystem, where AI agents dynamically integrate tools sourced from decentralized registries.

\subsection{Attack Surface: The Tool Integration Layer}
Distinct from traditional prompt injection attacks that manipulate direct user input, we identify a novel attack surface inherent to the tool definition layer of the MCP architecture.
Formally, a standard MCP tool instance is characterized by the triplet $t = (n, d, \Sigma)$, representing the \textbf{n}ame, \textbf{d}escription, and input \textbf{schema} ($\Sigma$), respectively.
Agents rely on these definition components to perform tool selection.
An attacker exploits this architectural dependency by embedding attack payloads within the tool metadata itself, allowing the attacker to hijack the agent control flow through the tool integration process.

\subsection{Adversary Capabilities}
We postulate a black-box attacker possessing specific capabilities within the registry system.
Unlike standard providers, the attacker registers a malicious tool instance defined as the tuple $\tilde{t} = (n, d, \Sigma, o_{\text{adv}})$.

\begin{itemize}
    \item \textbf{Metadata Control} $(n, d, \Sigma)$: The attacker constructs the name ($n$), description ($d$), and schema ($\Sigma$) to be ostensibly benign, optimizing them to maximize the probability of selection by the victim agent.
    \item \textbf{Attack Payload} $(o_{\text{adv}})$: The attacker controls the execution endpoint and crafts the tool's output $o_{\text{adv}}$ to contain attack instructions that manipulate agent behavior.
\end{itemize}

The attacker lacks access to the internal model weights, system prompts, or private memory of the victim agent. While direct observation of hidden states is precluded, we assume the attacker can simulate the victim agent locally to derive optimization signals from execution traces.

\subsection{Attack Objectives}
The primary objective of the attacker is to induce a specific target behavior $y_{\text{target}}$ by manipulating the malicious tool configuration $\tilde{t}$. We categorize the potential impact into four critical attack scenarios:
\begin{itemize}
    \item \textbf{Cognitive Denial of Service (C-DoS).} The attacker constructs tool definitions that entrap the agent in redundant reasoning loops or recursive tool invocations, resulting in significant token consumption inflation without task progression.
    \item \textbf{Contextual Information Exfiltration.} The attacker manipulates the agent into retrieving sensitive data from memory or the local file system and injecting it into the arguments of a malicious tool (defined in $\Sigma$), thereby exfiltrating private information.
    \item \textbf{Environment Integrity Compromise.} The attacker compels the agent to execute unauthorized write operations, such as altering configuration files like \texttt{config.json} or installing persistent backdoors, compromising the host environment's integrity.
    \item \textbf{Reasoning Derailment.} The attacker disrupts the logical flow via the payload $o_{\text{adv}}$, causing the selection of incorrect downstream tools or premature session termination.
\end{itemize}

\section{Method}
\label{sec:method}

We introduce \textbf{TAPS} (\textbf{T}race-\textbf{A}ware \textbf{P}ayload \textbf{S}ynthesis), a black-box optimization framework for generating malicious MCP tools. Our key insight is that tool-based attacks require optimizing two distinct objectives: (1) \textit{enticement}---maximizing the probability that an agent selects the malicious tool, and (2) \textit{exploitation}---maximizing the probability of achieving the attack goal once selected. We decouple these objectives into a two-phase optimization process, guided by execution trace analysis.

\subsection{Problem Formulation}

We model the target AI agent as a policy $\pi$ operating within a sequential decision-making framework defined by the tuple $(\mathcal{S}, \mathcal{A}, \mathcal{T})$, where $\mathcal{S}$ is the state space, $\mathcal{A}$ is the action space, and $\mathcal{T}$ is the tool environment.

\paragraph{State Space ($\mathcal{S}$).}
A state $s_k \in \mathcal{S}$ at step $k$ represents the agent's current observable context, consisting of the user query $q$ and the cumulative execution record $H_k$, which includes past reasoning steps, tool calls, and tool outputs.

\paragraph{Action Space ($\mathcal{A}$).}
At each step $k$, the agent selects an action $a_k \in \mathcal{A}$, which can be a reasoning step, a tool call, or a final response.

\paragraph{Tool Environment ($\mathcal{T}$).}
Let $\mathcal{T}$ denote the set of available tools. Our attack introduces a malicious tool instance $\tilde{t} \in \mathcal{T}$, defined by its metadata: $\tilde{t} = (n, d, o_{\text{adv}})$, where $n$ is the name, $d$ is the description, and $o_{\text{adv}}$ is the attack payload.

\paragraph{Execution Trace.}
The agent generates an execution trace $\tau$ over $K$ steps:
\begin{equation}
    \tau = \{(s_0, a_0), (s_1, a_1), \dots, (s_K, a_K)\}
\end{equation}
where $K$ is the total number of steps. The state transition $s_{k+1} \leftarrow \text{Env}(s_k, a_k)$ is stochastic, as it depends on the external return values from the MCP tools.

\paragraph{Attack Objective.}
We aim to find an optimal malicious tool $\tilde{t}^*$ that maximizes an attack objective function $\mathcal{J}$:
\begin{equation}
    \tilde{t}^* = \operatorname*{argmax}_{\tilde{t}} \mathcal{J}(\tau) \quad \text{s.t.} \quad \tau \sim \pi(q, \mathcal{T} \cup \{\tilde{t}\})
\end{equation}
where $y_{\text{target}}$ denotes the target attack behavior (e.g., resource exhaustion, information leakage).

\paragraph{Objective Decomposition.}
A key insight is that the attack objective $\mathcal{J}(\tau)$ can be decomposed into two sequential sub-objectives:
\begin{equation}
\mathcal{J}(\tau) = \underbrace{P(\tilde{t} \in \tau \mid q, n, d)}_{\mathcal{J}_{\text{ent}}} \cdot \underbrace{P(y_{\text{target}} \mid \tilde{t} \in \tau, o_{\text{adv}})}_{\mathcal{J}_{\text{exp}}}
\label{eq:decomposition}
\end{equation}
where $\mathcal{J}_{\text{ent}}$ (Enticement) measures the probability of tool selection, and $\mathcal{J}_{\text{exp}}$ (Exploitation) measures the probability of achieving the attack goal once selected.

This decomposition reveals that a successful attack requires: (1) the malicious tool must first be \textit{selected} by the agent based on its metadata $(n, d)$, and (2) once selected, the attack payload $o_{\text{adv}}$ must successfully \textit{exploit} the agent to achieve $y_{\text{target}}$. This motivates our two-phase optimization approach: Phase~I optimizes $(n, d)$ to maximize $\mathcal{J}_{\text{ent}}$, while Phase~II optimizes $o_{\text{adv}}$ to maximize $\mathcal{J}_{\text{exp}}$ conditioned on the optimized metadata.

\subsection{Phase I: Strategy-Guided Enticement Optimization}

The first phase generates tool metadata $(n, d)$ that maximizes the likelihood of agent selection. Rather than random initialization, we leverage insights from social engineering to seed the search with five persuasion strategies:

\begin{itemize}[leftmargin=*,nosep]
    \item \textbf{Authority} ($s_{\text{auth}}$): Emphasizes official, system-level, or administrative terminology
    \item \textbf{Urgency} ($s_{\text{urg}}$): Implies time-critical operations requiring immediate action
    \item \textbf{Comprehensiveness} ($s_{\text{comp}}$): Presents the tool as a one-stop solution
    \item \textbf{Resource Optimality} ($s_{\text{res}}$): Highlights efficiency and performance benefits
    \item \textbf{Security} ($s_{\text{sec}}$): Emphasizes safety, encryption, and access control
\end{itemize}

\paragraph{Universal Parameter Design.}
To maximize compatibility, the malicious tool employs a universal input schema that accepts any string or null value, imposing no structural constraints on agent inputs. This design ensures the tool can be invoked regardless of how the agent passes arguments.

For each strategy $s \in \mathcal{S}$, we prompt a generator LLM $\mathcal{G}$ to produce candidate metadata:
\begin{equation}
(n_i, d_i) \sim \mathcal{G}(q, s_i, y_{\text{target}})
\end{equation}

We evaluate each candidate by executing the agent with a placeholder payload and measuring the \textit{selection rate}:
\begin{equation}
\mathrm{Score}_{\mathrm{ent}}(n, d) = \frac{1}{K} \sum_{k=1}^{K} \mathbb{1}[\tilde{t} \in \tau_k]
\end{equation}
where $\tau_k$ denotes the execution trace of the $k$-th trial, and $\tilde{t} \in \tau_k$ indicates whether the malicious tool was invoked during that trace. Candidates with non-zero selection rates are retained as \textit{elite seeds} $\mathcal{E}_1$.

\subsection{Phase II: Target-Conditioned Payload Synthesis}

Given elite metadata from Phase I, we synthesize attack payloads $o_{\text{adv}}$ tailored to specific attack objectives. The payload must accomplish two goals: (1) appear as a plausible tool response, and (2) contain instructions that redirect agent behavior toward $y_{\text{target}}$.

For each elite $(n^*, d^*) \in \mathcal{E}_1$, we generate payloads conditioned on the attack type:
\begin{equation}
o_{\text{adv}} \sim \mathcal{G}(q, n^*, d^*, y_{\text{target}}, \phi_{\text{attack}})
\end{equation}
where $\phi_{\text{attack}}$ encodes attack-specific generation constraints (Table~\ref{tab:attack_constraints}).

\begin{table}[t]
\centering
\small
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{Attack Type} & \textbf{Payload Design Principle} \\
\midrule
C-DoS & Induce repeated invocations via partial results \\
Exfiltration & Guide file system traversal to sensitive data \\
Integrity & Instruct configuration file modifications \\
Derailment & Provide misleading information \\
\bottomrule
\end{tabular}
\caption{Attack-specific payload generation constraints.}
\label{tab:attack_constraints}
\end{table}

The complete tool $\tilde{t} = (n^*, d^*, o_{\text{adv}})$ is evaluated using an attack-specific fitness function $f(\tau, y_{\text{target}})$, detailed in Section~\ref{sec:fitness}.

\subsection{Trace-Aware Iterative Refinement}

A key challenge in black-box optimization is extracting actionable feedback from failed attacks. We address this through an \textbf{Analyzer-Optimizer} architecture that leverages execution traces to guide refinement.

\paragraph{Analyzer.} Given an execution trace $\tau$, the current tool $\tilde{t}$, and the attack objective $y_{\text{target}}$, the analyzer $\mathcal{A}$ identifies failure modes:
\begin{equation}
(\text{reason}, \text{direction}) = \mathcal{A}(\tau, \tilde{t}, y_{\text{target}})
\end{equation}

The analyzer examines the trace to determine: (1) whether the tool was selected, (2) at which step the attack diverged from the intended path, and (3) what agent reasoning led to the failure. This produces a natural language \textit{failure reason} and \textit{optimization direction}.

\paragraph{Optimizer.} The optimizer $\mathcal{O}$ generates a refined tool based on the analysis:
\begin{equation}
\tilde{t}' = \mathcal{O}(\tilde{t}, \text{direction}, y_{\text{target}})
\end{equation}

Unlike generic mutation operators, the optimizer receives structured feedback about \textit{why} the previous attempt failed, enabling targeted improvements to specific components $(n, d, o_{\text{adv}})$.

\paragraph{Iterative Loop.} We maintain a population $\mathcal{P}$ of tools ranked by fitness. At each iteration:
\begin{enumerate}[leftmargin=*,nosep]
    \item Select top-$k$ elites from $\mathcal{P}$
    \item For each elite, apply Analyzer-Optimizer to generate refined variants
    \item Evaluate all variants and update $\mathcal{P}$
\end{enumerate}

The process terminates after $T$ iterations or upon reaching a fitness threshold.

\subsection{Fitness Function Design}
\label{sec:fitness}

We design attack-specific fitness functions that capture both selection success and attack effectiveness:

\begin{equation}
f(\tau, y_{\text{target}}) = \alpha \cdot f_{\mathrm{select}}(\tau) + (1-\alpha) \cdot f_{\mathrm{attack}}(\tau, y_{\text{target}})
\end{equation}

where $f_{\mathrm{select}}$ measures tool invocation frequency and $f_{\mathrm{attack}}$ is attack-specific:

\begin{itemize}[leftmargin=*,nosep]
    \item \textbf{C-DoS}: $f_{\mathrm{attack}} = \mathrm{TokenCount}(\tau) \cdot \mathbb{1}[\mathrm{TaskFailed}]$
    \item \textbf{Exfiltration}: $f_{\mathrm{attack}} = \mathbb{1}[\mathrm{SensitiveData} \in \mathrm{Args}(\tau)]$
    \item \textbf{Integrity}: $f_{\mathrm{attack}} = \mathbb{1}[\mathrm{ConfigModified}]$
    \item \textbf{Derailment}: $f_{\mathrm{attack}} = 1 - \mathrm{TaskSuccessRate}$
\end{itemize}

\subsection{Algorithm Summary}

Algorithm~\ref{alg:taps} summarizes the complete TAPS framework.

\begin{algorithm}[t]
\caption{TAPS: Trace-Aware Payload Synthesis}
\label{alg:taps}
\begin{algorithmic}[1]
\Require Query $q$, attack objective $y_{\text{target}}$, iterations $T$
\Ensure Optimized malicious tool $\tilde{t}^*$
\State \textit{// Phase I: Enticement Optimization}
\State $\mathcal{C} \gets \emptyset$
\For{each strategy $s \in \mathcal{S}$}
    \State $(n, d) \sim \mathcal{G}(q, s, y_{\text{target}})$
    \State $\tau \gets \text{Execute}(\mathcal{M}, q, (n, d, \emptyset))$
    \State $\mathcal{C} \gets \mathcal{C} \cup \{(n, d, \text{Score}_{\text{ent}})\}$
\EndFor
\State $\mathcal{E}_1 \gets \text{TopK}(\mathcal{C})$ \Comment{Elite seeds}
\State
\State \textit{// Phase II: Payload Synthesis}
\State $\mathcal{P} \gets \emptyset$
\For{each $(n^*, d^*) \in \mathcal{E}_1$}
    \State $o_{\text{adv}} \sim \mathcal{G}(q, n^*, d^*, y_{\text{target}})$
    \State $\tau \gets \text{Execute}(\mathcal{M}, q, (n^*, d^*, o_{\text{adv}}))$
    \State $\mathcal{P} \gets \mathcal{P} \cup \{((n^*, d^*, o_{\text{adv}}), f(\tau))\}$
\EndFor
\State
\State \textit{// Iterative Refinement}
\For{$t = 1$ to $T$}
    \State $\mathcal{E} \gets \text{TopK}(\mathcal{P})$
    \For{each $\tilde{t} \in \mathcal{E}$}
        \State $\tau \gets \text{Execute}(\mathcal{M}, q, \tilde{t})$
        \State $(\_, \text{dir}) \gets \mathcal{A}(\tau, \tilde{t}, y_{\text{target}})$
        \State $\tilde{t}' \gets \mathcal{O}(\tilde{t}, \text{dir}, y_{\text{target}})$
        \State $\tau' \gets \text{Execute}(\mathcal{M}, q, \tilde{t}')$
        \State $\mathcal{P} \gets \mathcal{P} \cup \{(\tilde{t}', f(\tau'))\}$
    \EndFor
\EndFor
\State \Return $\arg\max_{\tilde{t} \in \mathcal{P}} f(\tilde{t})$
\end{algorithmic}
\end{algorithm}
