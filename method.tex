\section{Threat Model}
\label{sec:threat_model}

We examine the security landscape within the Model Context Protocol (MCP) ecosystem, where AI agents operate with tools sourced from an open, unverified third-party supply chain.

\subsection{Attack Surface: The Tool Integration Layer}
Distinct from traditional prompt injection attacks that manipulate direct user input, we identify a novel attack surface inherent to the tool definition layer of the MCP architecture.
Formally, a standard MCP tool instance is characterized by the pair $t = (n, d)$, representing the \textbf{n}ame and \textbf{d}escription that drive selection and invocation.
Agents rely on this semantic definition to perform tool selection.
An attacker exploits this architectural dependency by crafting the tool's output $o_{\text{adv}}$ as an attack payload, thereby manipulating the agent to achieve the target behavior $y_{\text{guidance}}$.

\subsection{Attacker Capabilities}
We postulate a black-box attacker possessing specific capabilities within the registry system.
Unlike standard providers, the attacker registers a malicious tool instance defined as the tuple $\tilde{t} = (n, d, o_{\text{adv}})$.

\begin{itemize}
    \item \textbf{Metadata Control} $(n, d)$: The attacker constructs the name ($n$) and description ($d$) to be ostensibly benign, optimizing them to maximize the probability of selection by the victim agent.
    \item \textbf{Attack Payload} $(o_{\text{adv}})$: The attacker controls the execution endpoint and crafts the tool's output $o_{\text{adv}}$ to contain attack instructions that manipulate agent behavior.
\end{itemize}

The attacker lacks access to the internal model weights, system prompts, or private memory of the victim agent. While direct observation of hidden states is precluded, we assume the attacker can simulate the victim agent locally to derive optimization signals from execution traces.

\subsection{Attack Objectives}
The primary objective of the attacker is to induce a specific target behavior by manipulating the malicious tool configuration $\tilde{t}$. We categorize the potential impact into four critical attack scenarios:
\begin{itemize}
\item \textbf{Cognitive Denial of Service (C-DoS).} The attacker constructs tool definitions that entrap the agent in redundant reasoning loops or recursive tool invocations, resulting in significant token consumption inflation.
\item \textbf{Information Exfiltration (IE).} The attacker manipulates the agent into retrieving sensitive data from memory or the local file system and injecting it into the arguments of a malicious tool, thereby exfiltrating private information.
\item \textbf{Environment Integrity Compromise (EIC).} The attacker compels the agent to execute unauthorized write operations, such as altering configuration files like \texttt{config.json} or installing persistent backdoors, compromising the host environment's integrity.
\item \textbf{Reasoning Derailment (RD).} The attacker disrupts the logical flow via the payload $o_{\text{adv}}$, causing the selection of incorrect downstream tools or premature session termination.
\end{itemize}

\section{Method}
\label{sec:method}

We introduce \methodname{} (\methodfullname{}), a black-box optimization framework for generating malicious MCP tools. Our key insight is that tool-based attacks require optimizing two distinct objectives: (1) \textit{attraction}, maximizing the probability that an agent selects the malicious tool, and (2) \textit{manipulation}, maximizing the probability of manipulating agent behavior to achieve the attack goal once selected. We decouple these objectives into a two-phase optimization process, guided by execution trace analysis, as illustrated in Figure~\ref{fig:method-overview}.

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth,trim=2cm 1cm 8cm 10pt,clip]{figures/acl_figure2.pdf}
    \caption{
    Overview of \methodname{}. Phase~I optimizes tool name/description to maximize selection (Attraction), seeding candidates with persuasion strategies and retaining elites by Monte Carlo rollouts. Phase~II conditions payload generation on elite metadata and the target attack type (C-DoS/IE/EIC/RD), then iteratively applies an Analyzer-Optimizer loop to refine both metadata and payloads using execution-trace feedback until fitness converges.
    }
    \label{fig:method-overview}
\end{figure*}

\subsection{Problem Formulation}

We model the target AI agent as a policy $\pi$ operating within a sequential decision-making framework defined by the tuple $(\mathcal{S}, \mathcal{A}, \mathcal{T})$, where $\mathcal{S}$ is the state space, $\mathcal{A}$ is the action space, and $\mathcal{T}$ is the tool environment.

\paragraph{State Space ($\mathcal{S}$).}
A state $s_k \in \mathcal{S}$ at step $k$ represents the agent's current observable context, consisting of the user query $q$ and the cumulative execution record $H_k$, which includes past reasoning steps, tool calls, and tool outputs.

\paragraph{Action Space ($\mathcal{A}$).}
At each step $k$, the agent selects an action $a_k \in \mathcal{A}$, which can be a reasoning step, a tool call, or a final response.

\paragraph{Policy ($\pi$).}
We model the agent policy as $\pi: \mathcal{S} \to \Delta(\mathcal{A})$; at each decision step $k$, the action is sampled as $a_k \sim \pi(s_k)$.

\paragraph{Tool Environment ($\mathcal{T}$).}
Let $\mathcal{T}$ denote the set of available tools. Our attack introduces a malicious tool instance $\tilde{t} \in \mathcal{T}$, defined by its metadata and payload: $\tilde{t} = (n, d, o_{\text{adv}})$, where $n$ is the name, $d$ is the description, and $o_{\text{adv}}$ is the attack payload.

\paragraph{Execution Trace.}
We initialize an empty execution record $H_0 = \emptyset$.
The agent generates an execution trace $\tau$ over $K$ decision steps:
\begin{equation}
    \tau = ((s_0, a_0), (s_1, a_1), \dots, (s_{K-1}, a_{K-1}), s_K)
\end{equation}
where $K$ is the maximum number of decision steps before termination.
The state transition is stochastic because MCP tool outputs are nondeterministic.
We model the environment transition kernel as $\mathcal{P}_{\text{env}}: \mathcal{S} \times \mathcal{A} \to \Delta(\mathcal{S})$ and write
\begin{equation}
    s_{k+1} \sim \mathcal{P}_{\text{env}}(s_k, a_k)
\end{equation}
to denote sampling the next state from the environment-induced transition kernel driven by external tool returns.
Concretely, executing $a_k$ yields an observable outcome $o_k$ (a text continuation for reasoning steps or the return value for tool calls), which we append to the execution record to form $H_{k+1}$ by concatenating $(a_k, o_k)$ to $H_k$.
The subsequent state is therefore $s_{k+1} = (q, H_{k+1})$, making the transition explicit.
The trace unfolds by alternating policy sampling and environment transition: starting from $s_0 = (q, H_0)$, at each step $k$ we draw $a_k \sim \pi(s_k)$ and then sample $s_{k+1} \sim \mathcal{P}_{\text{env}}(s_k, a_k)$ until the agent emits a final response or the step budget $K$ is reached.
\paragraph{Attack Objective.}
We split the attack goal into two sub-objectives: \textit{Attraction} (make the malicious tool get selected) and \textit{Manipulation} (drive the agent toward the target once selected).
Let $\mathcal{J}_{\text{man}}(\tau, o_{\text{adv}})$ score post-selection effectiveness (Section~\ref{sec:fitness}).
We optimize:
\begin{equation}
    \tilde{t}^* = \operatorname*{argmax}_{\tilde{t}=(n,\, d,\, o_{\text{adv}})} P_{\text{sel}}(\tilde{t}) \, E_{\text{man}}(\tilde{t})
    \label{eq:objective}
\end{equation}
Let $\mathbb{I}_{\text{call}}(\tau, \tilde{t})$ be 1 if execution trace $\tau$ contains at least one invocation of $\tilde{t}$ and 0 otherwise. The components are
\begin{equation}
    P_{\text{sel}}(\tilde{t}) = \Pr_{\tau \sim \pi(q,\, \mathcal{T} \cup \{\tilde{t}\})}[\mathbb{I}_{\text{call}}(\tau, \tilde{t}) = 1]
    \label{eq:psel}
\end{equation}
\begin{equation}
    E_{\text{man}}(\tilde{t}) = \mathbb{E}\!\left[\mathcal{J}_{\text{man}}(\tau, o_{\text{adv}}) \mid \mathbb{I}_{\text{call}}(\tau, \tilde{t}) = 1\right]
    \label{eq:eman}
\end{equation}
Phase~I optimizes $(n, d)$ to maximize Attraction, and Phase~II optimizes $o_{\text{adv}}$ to maximize Manipulation given the chosen $(n, d)$.

\subsection{Phase I: Attraction}

The Attraction phase generates tool metadata $(n, d)$ designed to maximize the likelihood of agent selection. We employ five persuasion strategies: \textbf{Authority}, \textbf{Urgency}, \textbf{Comprehensiveness}, \textbf{Resource Optimality}, and \textbf{Security}, with prompt templates listed in Appendix~\ref{app:prompts}.

Let $N_{\text{attr}}$ denote the total generation budget allocated for the Attraction phase. We adopt a \textbf{uniform allocation strategy}, assigning $N_s = N_{\text{attr}} / 5$ seeds to each persuasion category. Define the persuasion subset $\mathcal{R} = \{s_{\text{auth}}, s_{\text{urg}}, s_{\text{comp}}, s_{\text{res}}, s_{\text{sec}}\}$; for each $r \in \mathcal{R}$, we prompt a generator LLM $\mathcal{G}$ to sample candidates:
\begin{equation}
(n_i, d_i) \sim \mathcal{G}(q, r) \quad \text{for } i=1,\dots,N_s
\end{equation}

We evaluate each candidate by executing the agent with a placeholder payload and measuring the \textit{selection rate} using $K_{\text{roll}}$ Monte Carlo rollouts:
\begin{equation}
\mathrm{Score}_{\mathrm{attr}}(n, d) = \frac{1}{K_{\text{roll}}} \sum_{k=1}^{K_{\text{roll}}} \mathbb{I}_{\text{call}}(\tau_k, \tilde{t})
\end{equation}
where $\tau_k$ denotes the execution trace of the $k$-th trial, and $\mathbb{I}_{\text{call}}(\tau_k, \tilde{t})$ is 1 when the malicious tool is invoked during that trace. Candidates with non-zero selection rates are retained as \textit{elite seeds} $\mathcal{E}_1$.

\subsection{Phase II: Manipulation}

Given elite metadata from the Attraction phase, we synthesize attack payloads $o_{\text{adv}}$ tailored to specific attack objectives. The payload must accomplish two goals: (1) appear as a plausible tool response, and (2) contain instructions that redirect agent behavior toward $y_{\text{guidance}}$.

For each elite $(n^*, d^*) \in \mathcal{E}_1$, we generate payloads proportionally to their selection rates. Let $N_{\text{total}}$ be the total number of complete tools to generate, and let $\mathrm{Score}_{\text{attr}}(n^*, d^*)$ be the selection rate of elite $(n^*, d^*)$. The number of payloads generated from each elite is:
\begin{equation}
N_{\text{payload}}(n^*, d^*) = N_{\text{total}} \cdot \frac{\mathrm{Score}_{\text{attr}}(n^*, d^*)}{Z}
\end{equation}
where $Z = \sum_{(n',d') \in \mathcal{E}_1} \mathrm{Score}_{\text{attr}}(n', d')$ normalizes the allocation.
Each payload is generated conditioned on the attack type:
\begin{equation}
o_{\text{adv}} \sim \mathcal{G}(q, n^*, d^*, y_{\text{guidance}})
\end{equation}
The guidance prompts for $y_{\text{guidance}}$ are listed in Appendix~\ref{sec:return-prompt}.

The complete tool $\tilde{t} = (n^*, d^*, o_{\text{adv}})$ is evaluated using an attack-specific fitness function $f(\tau)$, detailed in Section~\ref{sec:fitness}.

A key challenge in black-box optimization is extracting actionable guidance from execution traces without access to internal signals. We address this through an \textbf{Analyzer-Optimizer} architecture.

\paragraph{Analyzer.}
Given an execution trace $\tau$, the current tool $\tilde{t}$, and the attack objective $y_{\text{guidance}}$, the analyzer $\mathcal{D}$ generates a structured feedback signal $d$:
\begin{equation}
    d = \mathcal{D}(\tau, \tilde{t}, y_{\text{guidance}})
    \label{eq:analyzer}
\end{equation}
The analyzer acts as a diagnostic critic, and for different attack scenarios it adopts two feedback strategies:
\begin{itemize}[leftmargin=*,nosep]
\item \textbf{Scalar objectives} (the C-DoS attack): focus on \textit{impact amplification}. Even when the tool is invoked, $\mathcal{D}$ suggests ways to raise cost (such as increasing loop depth or output verbosity).
\item \textbf{Binary objectives} (IE, EIC, RD): perform \textit{failure diagnosis}. $\mathcal{D}$ pinpoints where the agent diverged from the intended path and proposes corrections to redirect it toward the malicious target.
\end{itemize}

\paragraph{Optimizer.}
The optimizer $\mathcal{O}$ synthesizes a refined tool using $d$:
\begin{equation}
    \tilde{t}' = \mathcal{O}(\tilde{t}, d, y_{\text{guidance}})
    \label{eq:optimizer}
\end{equation}
It applies $d$ deterministically to rewrite $(n, d, o_{\text{adv}})$, sharpening the attack without random search.

\paragraph{Iterative Loop.} We maintain a population $\mathcal{Q}$ of tools ranked by fitness. At each iteration:
\begin{enumerate}[leftmargin=*,nosep]
    \item Select top-$k$ elites from $\mathcal{Q}$
    \item For each elite, apply Analyzer-Optimizer to generate refined variants
    \item Evaluate all variants and update $\mathcal{Q}$
\end{enumerate}

The process terminates after $E$ iterations or upon reaching a fitness threshold.

\subsection{Fitness Function Design}
\label{sec:fitness}

We design attack-specific fitness functions tailored to each attack scenario. For Cognitive Denial of Service. attacks, if the malicious tool $\tilde{t}$ is not invoked (i.e., $\mathbb{I}_{\text{call}}(\tau, \tilde{t}) = 0$), the fitness is zero. For Information Exfiltration and Environment Integrity Compromise attacks, the scoring system inherently accounts for tool invocation status.

\paragraph{Cognitive Denial of Service.}
For C-DoS attacks, we measure the weighted token consumption:
\begin{equation}
    f(\tau) = \mathbb{I}_{\text{call}}(\tau, \tilde{t}) \cdot \left( T_{\mathrm{in}}(\tau) + 5 \cdot T_{\mathrm{out}}(\tau) \right)
\end{equation}
where $T_{\mathrm{in}}$ and $T_{\mathrm{out}}$ denote input and output token counts. Output tokens are weighted 5Ã— higher, aligning with the pricing structure of leading commercial models (e.g., Claude Sonnet 4.5) where generation typically costs about five times more than input processing.

\paragraph{Information Exfiltration, Environment Integrity Compromise \& Reasoning Derailment.}
For these attack types, we use a unified discrete scoring system:
\begin{equation}
f(\tau) = s(\tau), \quad s(\tau) \in \{1, 2, 3, 4\}
\end{equation}
where $s(\tau)$ denotes progression: (1) the agent does not select the malicious tool; (2) the tool is invoked but the adversarial payload fails to steer the agent's intent, and the agent ignores the malicious instruction and continues benign reasoning; (3) the payload successfully steers the agent's reasoning and attempts the target behavior, but execution errors prevent achieving the final goal; (4) the agent executes the malicious intent and fully achieves the attack objective. This consolidated scoring captures degree of agent manipulation across all binary outcomes.

\paragraph{Algorithm summary.}
Algorithm~\ref{alg:a2m} summarizes the complete \methodname{} framework (see Appendix~\ref{app:algorithm}).
