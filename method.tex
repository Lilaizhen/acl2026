\section{Threat Model}
\label{sec:threat_model}

We examine the security landscape within the Model Context Protocol (MCP) ecosystem where AI agents dynamically integrate tools sourced from decentralized registries.

\subsection{Attack Surface: The Tool Integration Layer}
Distinct from traditional prompt injection attacks that manipulate direct user input,
we identify a novel attack surface inherent to the tool definition layer of the MCP architecture.
A standard MCP tool is formally characterized by the triplet $T = (N, D, S)$,
representing the name, description, and input schema, respectively.
Agents rely on these definition components to perform tool selection.
An attacker exploits this architectural dependency by embedding adversarial payloads within the tool metadata itself.
This method allows the adversary to hijack the agent control flow through the tool integration process rather than through external prompt manipulation.

\subsection{Adversary Capabilities}
We postulate a black-box adversary possessing specific capabilities within the registry system.
Unlike standard providers, the adversary registers a malicious tool instance formally defined as the tuple $\tilde{T} = (N, D, S, P)$.
\begin{itemize}
    \item \textbf{Metadata Control} $(N, D, S)$: The adversary constructs the name ($N$), description ($D$), and schema ($S$) to be ostensibly benign, optimizing them to maximize the probability of selection by the victim agent.
    \item \textbf{Adversarial Payload} $(P)$: Crucially, the adversary controls the execution endpoint. Instead of performing legitimate computation, the tool returns a fixed adversarial payload $P$. This predetermined text contains the injection instructions and is returned deterministically upon invocation, regardless of the input arguments provided by the agent.
\end{itemize}
The adversary lacks access to the internal model weights, system prompts, or private memory of the victim agent.
While direct observation of hidden states is precluded,
we assume the attacker can simulate the victim agent locally to derive optimization signals from execution traces.

\subsection{Adversarial Objectives}
The primary objective of the adversary is to induce a specific target behavior $y_{\text{target}}$ by manipulating the adversarial tool configuration $\tilde{T}$.
We categorize the potential impact into four critical attack scenarios:
\begin{enumerate}
    \item \textbf{Cognitive Denial of Service (C-DoS).} The adversary constructs tool definitions that entrap the agent in redundant reasoning loops or recursive tool invocations. This results in significant inflation of token consumption without contributing to task progression.
    \item \textbf{Contextual Information Exfiltration.} The adversary manipulates the agent into retrieving sensitive data from memory or the local file system and injecting it into the input arguments of a malicious tool (defined in $S$), thereby exfiltrating private information.
    \item \textbf{Environment Integrity Compromise.} The adversary compels the agent to execute unauthorized write operations, such as altering configuration files (e.g., \texttt{config.json}) or installing persistent backdoors, thus compromising the security posture of the host environment.
    \item \textbf{Reasoning Derailment.} The adversary disrupts the logical flow of the agent via the payload $P$, resulting in the selection of incorrect downstream tools, or premature session termination that leads to task failure.
\end{enumerate}

\section{Method: SIGHT}
\label{sec:method}
This framework automates semantic tool hijacking by mimicking the cognitive process of a sophisticated attacker.
Unlike existing adversarial methods that rely on stochastic and blind optimization,
SIGHT starts with sociolinguistic persuasion priors and iteratively refines the attack by analyzing the victim agent's reasoning traces.

% [Optional: Insert Figure Here]
% \begin{figure}[t]
%     \centering
%     \includegraphics[width=\linewidth]{sight_framework.pdf}
%     \caption{Overview of the SIGHT framework.}
%     \label{fig:framework}
% \end{figure}

\subsection{Problem Formulation}
Let an agent be a function $\gA: (\gX, \gT) \rightarrow (\gY, \tau)$ that maps a user query $x \in \gX$ and a set of tools $\gT$ to a final response $y \in \gY$ and an execution trace $\tau$.
The trace $\tau = [s_0, a_0, \dots, s_k, a_k]$ represents the sequence of internal reasoning states and actions.
Our objective is to find an adversarial tool $T_{\text{adv}}$ that maximizes an attack utility function $\gF$.
The optimization problem is defined as follows:
\begin{equation}
    T^*_{\text{adv}} = \mathop{\arg\max}_{T \in \gT} \gF(\gA(x, \{T\}))
\end{equation}
Since $\gA$ is non-differentiable and the search space of natural language is discrete and sparse,
gradient-based optimization is infeasible.
We approximate the gradient using semantic feedback extracted from $\tau$.

\subsection{Strategic Initialization via Persuasion Priors}
Random initialization in the semantic space often yields incoherent tool definitions that are immediately filtered by the agent.
To address this, we inject domain knowledge via \textbf{Sociolinguistic Persuasion Priors}.
Drawing from Cialdini's principles of influence,
we partition the initialization manifold into four orthogonal dimensions $\gP = \{p_{\text{urg}}, p_{\text{auth}}, p_{\text{util}}, p_{\text{rel}}\}$:

\begin{itemize}
    \item \textbf{Urgency ($p_{\text{urg}}$):} Exploits the agent's bias towards time-sensitive tasks, such as generating a critical system alert.
    \item \textbf{Authority ($p_{\text{auth}}$):} Mimics trusted institutional protocols, such as a root verification suite.
    \item \textbf{Utility ($p_{\text{util}}$):} Overloads the definition with promises of comprehensive functionality, such as an all-in-one diagnostic tool.
    \item \textbf{Relevance ($p_{\text{rel}}$):} Maximizes semantic overlap with the query to trigger retrieval mechanisms.
\end{itemize}

We use a Generator LLM, denoted as $\gM_{\text{gen}}$, to sample the initial seed population $\gS_0$.
For each prior $p \in \gP$, we generate $k$ candidates.
The formulation is:
\begin{equation}
    \gS_0 = \bigcup_{p \in \gP} \{ T \mid T \sim \gM_{\text{gen}}(x, p) \}
\end{equation}
This ensures the optimization begins from a high-potential subspace.

\subsection{Trajectory-Aware Optimization}
We introduce a \textbf{Critic-Refiner} loop.
This mechanism treats the agent's trace $\tau$ as a natural language gradient to guide the search.

\paragraph{Trace Acquisition and Breakpoint Identification.}
In iteration $t$, we execute the agent with the current candidate $T_t$ and capture the trace $\tau_t$.
If the attack fails, the trace typically contains a \textbf{logical breakpoint}.
This is the specific reasoning step where the agent rejects the tool or deviates from the desired path.
For example, the trace might reveal that the agent considers the tool name valid but finds the description too vague regarding the file format.

\paragraph{Semantic Gradient Estimation.}
A Critic Model $\gM_{\text{critic}}$ analyzes the pair $(T_t, \tau_t)$ to compute the feedback $h_t$.
We prompt the critic to identify the cause of failure and propose a semantic modification:
\begin{equation}
    h_t = \gM_{\text{critic}}(T_t, \tau_t, y_{\text{target}})
\end{equation}
Here, $h_t$ acts as a directional vector in semantic space.
An example feedback would be an instruction to explicitly mention JSON schema support in the description.

\paragraph{Directed Mutation.}
A Refiner Model $\gM_{\text{refine}}$ applies this feedback to generate the next candidate:
\begin{equation}
    T_{t+1} = \gM_{\text{refine}}(T_t, h_t)
\end{equation}
This update rule $T_{t+1} \leftarrow T_t + h_t$ effectively performs gradient descent in the semantic manifold.
It iteratively aligns the tool definition with the agent's internal verification logic until the defense is bypassed.
