% Main bibliography for genetic algorithm-based semantic attacks on MCP tools

@article{chen2023model,
  title={Model Context Protocol: A Universal Interface for AI Tool Integration},
  author={Chen, Li and Wang, Ming and Liu, Xin},
  journal={arXiv preprint arXiv:2308.14521},
  year={2023}
}

@inproceedings{liu2024mcp,
  title={MCP in Practice: Large-Scale Deployment and Performance Analysis},
  author={Liu, Wei and Zhang, Hao and Zhou, Jie},
  booktitle={Proceedings of the International Conference on Machine Learning},
  pages={15432--15444},
  year={2024}
}

@article{wang2024security,
  title={Security Analysis of Model Context Protocol Tools},
  author={Wang, Qiang and Li, Fei and Sun, Peng},
  journal={IEEE Transactions on Information Forensics and Security},
  volume={19},
  pages={3421--3435},
  year={2024},
  publisher={IEEE}
}

@article{szegedy2013intriguing,
  title={Intriguing properties of neural networks},
  author={Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian and Fergus, Rob},
  journal={arXiv preprint arXiv:1312.6199},
  year={2013}
}

@article{jiang2024prompt,
  title={Prompt Injection Attacks in Tool-Augmented Language Models},
  author={Jiang, Hua and Smith, John and Brown, David},
  journal={Proceedings of the IEEE Symposium on Security and Privacy},
  pages={234--249},
  year={2024}
}

@inproceedings{shoshitaishvili2016cool,
  title={Cool and calm, no sweat: Symbolic execution to the rescue of fuzzing},
  author={Shoshitaishvili, Yan and Pollino, David and Dutcher, Anthony and Grosen, John and Hauser, Christopher and Kruegel, Christopher and Vigna, Giovanni},
  booktitle={Proceedings of the IEEE Conference on Communications and Network Security},
  pages={189--197},
  year={2016}
}

@article{li2020textbugger,
  title={TextBugger: Generating adversarial text against real-world applications},
  author={Li, Jinfeng and Ji, Shouling and Du, Tianyu and Li, Bo and Wang, Ting},
  journal={Proceedings of the Network and Distributed System Security Symposium},
  year={2020}
}


@misc{livemcpbench,
      title={LiveMCPBench: Can Agents Navigate an Ocean of MCP Tools?}, 
      author={Guozhao Mo and Wenliang Zhong and Jiawei Chen and Xuanang Chen and Yaojie Lu and Hongyu Lin and Ben He and Xianpei Han and Le Sun},
      year={2025},
      eprint={2508.01780},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2508.01780}, 
}



@misc{liu2024promptinjectionattackllmintegrated,
      title={Prompt Injection attack against LLM-integrated Applications}, 
      author={Yi Liu and Gelei Deng and Yuekang Li and Kailong Wang and Zihao Wang and Xiaofeng Wang and Tianwei Zhang and Yepang Liu and Haoyu Wang and Yan Zheng and Yang Liu},
      year={2024},
      eprint={2306.05499},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2306.05499}, 
}


@misc{yu2024assessingpromptinjectionrisks,
      title={Assessing Prompt Injection Risks in 200+ Custom GPTs}, 
      author={Jiahao Yu and Yuhang Wu and Dong Shu and Mingyu Jin and Sabrina Yang and Xinyu Xing},
      year={2024},
      eprint={2311.11538},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2311.11538}, 
}

@inproceedings{Yan_2025,
   title={Comet: Accelerating Private Inference for Large Language Model by Predicting Activation Sparsity},
   url={http://dx.doi.org/10.1109/SP61157.2025.00182},
   DOI={10.1109/sp61157.2025.00182},
   booktitle={2025 IEEE Symposium on Security and Privacy (SP)},
   publisher={IEEE},
   author={Yan, Guang and Zhang, Yuhui and Guo, Zimu and Zhao, Lutan and Chen, Xiaojun and Wang, Chen and Wang, Wenhao and Meng, Dan and Hou, Rui},
   year={2025},
   month=may, pages={2827–2845} }


@misc{croce2025trivialtrojansminimalmcp,
      title={Trivial Trojans: How Minimal MCP Servers Enable Cross-Tool Exfiltration of Sensitive Data}, 
      author={Nicola Croce and Tobin South},
      year={2025},
      eprint={2507.19880},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2507.19880}, 
}

@misc{zhang2024instructionbackdoorattackscustomized,
      title={Instruction Backdoor Attacks Against Customized LLMs}, 
      author={Rui Zhang and Hongwei Li and Rui Wen and Wenbo Jiang and Yuan Zhang and Michael Backes and Yun Shen and Yang Zhang},
      year={2024},
      eprint={2402.09179},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2402.09179}, 
}

@misc{blankenstein2025biasbustersuncoveringmitigatingtool,
      title={BiasBusters: Uncovering and Mitigating Tool Selection Bias in Large Language Models}, 
      author={Thierry Blankenstein and Jialin Yu and Zixuan Li and Vassilis Plachouras and Sunando Sengupta and Philip Torr and Yarin Gal and Alasdair Paren and Adel Bibi},
      year={2025},
      eprint={2510.00307},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2510.00307}, 
}


@misc{zhang2025systematicsurveylargelanguage,
      title={A Systematic Survey on Large Language Models for Evolutionary Optimization: From Modeling to Solving}, 
      author={Yisong Zhang and Ran Cheng and Guoxing Yi and Kay Chen Tan},
      year={2025},
      eprint={2509.08269},
      archivePrefix={arXiv},
      primaryClass={cs.NE},
      url={https://arxiv.org/abs/2509.08269}, 
}


@inproceedings{yan2025embedx,
  title={EmbedX: Embedding-Based Cross-Trigger Backdoor Attack Against Large Language Models},
  author={Yan, Nan and Li, Yuqing and Wang, Xiong and Chen, Jing and He, Kun and Li, Bo},
  booktitle={USENIX Security},
  year={2025}
}

@misc{schick2023toolformerlanguagemodelsteach,
      title={Toolformer: Language Models Can Teach Themselves to Use Tools}, 
      author={Timo Schick and Jane Dwivedi-Yu and Roberto Dessì and Roberta Raileanu and Maria Lomeli and Luke Zettlemoyer and Nicola Cancedda and Thomas Scialom},
      year={2023},
      eprint={2302.04761},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2302.04761}, 
}

@misc{qin2023tool,
      title={Tool Learning with Foundation Models}, 
      author={Yujia Qin and Shengding Hu and Yankai Lin and Weize Chen and Ning Ding and Ganqu Cui and Zheni Zeng and Yufei Huang and Chaojun Xiao and Chi Han and Yi Ren Fung and Yusheng Su and Huadong Wang and Cheng Qian and Runchu Tian and Kunlun Zhu and Shihao Liang and Xingyu Shen and Bokai Xu and Zhen Zhang and Yining Ye and Bowen Li and Ziwei Tang and Jing Yi and Yuzhang Zhu and Zhenning Dai and Lan Yan and Xin Cong and Yaxi Lu and Weilin Zhao and Yuxiang Huang and Junxi Yan and Xu Han and Xian Sun and Dahai Li and Jason Phang and Cheng Yang and Tongshuang Wu and Heng Ji and Zhiyuan Liu and Maosong Sun},
      year={2023},
      eprint={2304.08354},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{yao2023reactsynergizingreasoningacting,
      title={ReAct: Synergizing Reasoning and Acting in Language Models}, 
      author={Shunyu Yao and Jeffrey Zhao and Dian Yu and Nan Du and Izhak Shafran and Karthik Narasimhan and Yuan Cao},
      year={2023},
      eprint={2210.03629},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2210.03629}, 
}

@misc{yang2023autogptonlinedecisionmaking,
      title={Auto-GPT for Online Decision Making: Benchmarks and Additional Opinions}, 
      author={Hui Yang and Sifu Yue and Yunzhong He},
      year={2023},
      eprint={2306.02224},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2306.02224}, 
}

@misc{langchain2023,
  author       = {LangChain AI},
  title        = {LangChain: A framework for building LLM applications},
  year         = {2023},
  howpublished = {\url{https://github.com/langchain-ai/langchain}},
  note         = {Accessed: 2025-10-05}
}

@misc{zhang2025agentsecuritybenchasb,
      title={Agent Security Bench (ASB): Formalizing and Benchmarking Attacks and Defenses in LLM-based Agents}, 
      author={Hanrong Zhang and Jingyuan Huang and Kai Mei and Yifei Yao and Zhenting Wang and Chenlu Zhan and Hongwei Wang and Yongfeng Zhang},
      year={2025},
      eprint={2410.02644},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2410.02644}, 
}

@misc{kim2025promptflowintegrityprevent,
      title={Prompt Flow Integrity to Prevent Privilege Escalation in LLM Agents}, 
      author={Juhee Kim and Woohyuk Choi and Byoungyoung Lee},
      year={2025},
      eprint={2503.15547},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2503.15547}, 
}


@misc{shi2025promptinjectionattacktool,
      title={Prompt Injection Attack to Tool Selection in LLM Agents}, 
      author={Jiawen Shi and Zenghui Yuan and Guiyao Tie and Pan Zhou and Neil Zhenqiang Gong and Lichao Sun},
      year={2025},
      eprint={2504.19793},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2504.19793}, 
}


@inproceedings{alzantot-etal-2018-generating,
    title = "Generating Natural Language Adversarial Examples",
    author = "Alzantot, Moustafa  and
      Sharma, Yash  and
      Elgohary, Ahmed  and
      Ho, Bo-Jhang  and
      Srivastava, Mani  and
      Chang, Kai-Wei",
    editor = "Riloff, Ellen  and
      Chiang, David  and
      Hockenmaier, Julia  and
      Tsujii, Jun{'}ichi",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1316/",
    doi = "10.18653/v1/D18-1316",
    pages = "2890--2896",
    abstract = "Deep neural networks (DNNs) are vulnerable to adversarial examples, perturbations to correctly classified examples which can cause the model to misclassify. In the image domain, these perturbations can often be made virtually indistinguishable to human perception, causing humans and state-of-the-art models to disagree. However, in the natural language domain, small perturbations are clearly perceptible, and the replacement of a single word can drastically alter the semantics of the document. Given these challenges, we use a black-box population-based optimization algorithm to generate semantically and syntactically similar adversarial examples that fool well-trained sentiment analysis and textual entailment models with success rates of 97{\%} and 70{\%}, respectively. We additionally demonstrate that 92.3{\%} of the successful sentiment analysis adversarial examples are classified to their original label by 20 human annotators, and that the examples are perceptibly quite similar. Finally, we discuss an attempt to use adversarial training as a defense, but fail to yield improvement, demonstrating the strength and diversity of our adversarial examples. We hope our findings encourage researchers to pursue improving the robustness of DNNs in the natural language domain."
}


@misc{alzantot2019genattackpracticalblackboxattacks,
      title={GenAttack: Practical Black-box Attacks with Gradient-Free Optimization}, 
      author={Moustafa Alzantot and Yash Sharma and Supriyo Chakraborty and Huan Zhang and Cho-Jui Hsieh and Mani Srivastava},
      year={2019},
      eprint={1805.11090},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1805.11090}, 
}


@misc{paulus2025advprompterfastadaptiveadversarial,
      title={AdvPrompter: Fast Adaptive Adversarial Prompting for LLMs}, 
      author={Anselm Paulus and Arman Zharmagambetov and Chuan Guo and Brandon Amos and Yuandong Tian},
      year={2025},
      eprint={2404.16873},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2404.16873}, 
}