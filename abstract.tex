\begin{abstract}
The evolution of Large Language Models into autonomous agents
via the Model Context Protocol (MCP)
introduces a critical security vulnerability
because agents rely on semantic matching
to select tools from unverified third-party MCP servers.
This creates a novel semantic supply-chain attack surface.
We introduce \textbf{\methodname{}} (\methodfullname{}),
a two-stage black-box optimization framework
designed to systematically hijack MCP agents.
\methodname{} first optimizes tool metadata
to maximize selection probability
through an Attraction phase
and subsequently employs a trace-driven optimizer
to craft adversarial return payloads that steer agent reasoning
towards attacker-desired outcomes
by optimizing the tool further during the Manipulation phase.
Extensive evaluations on LiveMCPBench tasks
across various frontier models,
including both proprietary and open-weight architectures,
demonstrate the severity of this threat.
\methodname{}
inflates token costs by up to $32.4\times$
under Cognitive Denial of Service,
and achieves high success rates
on Information Exfiltration, Environment Integrity Compromise, and Reasoning Derailment.
Furthermore,
optimized tools exhibit strong transferability to unseen models
and successfully bypass existing perplexity-based defenses
and lightweight auditors.
Our findings underscore the fragility of the tool selection layer
and highlight an urgent need
for robust vetting and isolation mechanisms
in agentic ecosystems.
% TODO: add code to supplementary material?
\methodname{} is available in an \href{https://anonymous.4open.science/r/A2M-63A0}{anonymous repository}.
\end{abstract}
